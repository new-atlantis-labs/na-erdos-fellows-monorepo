{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a database of the water in the Chesapeake Bay.\n",
    "\n",
    "The CSVs have the following columns:\n",
    "\n",
    "\n",
    "Here is a descriptor of the columns, from [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf):\n",
    "- `CBSeg2003` 2003 Chesapeake Bay Segment Designation. Divided into regions based on circulation and salinity properties. We used 8 from the Bay proper, 2 adjoining Bays, and 1 adjoining sound.\n",
    "- `CBSeg2003Description` 2003 Chesapeake Bay Segment Designation Description in the format Location-Salinity. The locations are Chesapeake Bay, Eastern Bay, Mobjack Bay, and Tangier Sound. The salinity levels are tidal fresh (0.0 - 0.5 parts per thousand),\n",
    "oligohaline (0.5 - 5.0 parts per thousand), mesohaline (5.0 - 18.0 parts per thousand), and polyhaline (greater than 18.0 parts per thousand). \n",
    "- `Station` the sampling station\n",
    "- `Latitude` and  `Longitude`, the Latitude and Longitude for the sampling station\n",
    "- `FieldActivityId` and `EventId` are Database Generated Event Identification Numbers. Theses values are distint-- `FieldActivityId` is adding from the monitoring event data. Sediment and have `EventId` as well.\n",
    "- `SampleDate` Sampling date (MM/DD/YYYY). \n",
    "- `SampleTime` Sample Collection Time (HHMM)\n",
    "- `Layer` Layer of Water Column in Which Sample Was Taken. However, this column is not consistently coded.\n",
    "- `TotalDepth` is Total Station Depth (Meters)\n",
    "- `Source` Data Collection Agency\n",
    "- `SampleReplicate` This parameter combines the sample replicate number with a sample type\n",
    "descriptor. \n",
    "     - S1, Sample 1. The vast majority of the data.\n",
    "     - S2, Sample 2 \n",
    "- `ReportingParameter` (sediment), `IBIParameter` (biomass) Sampling Parameter. In this dataset, all are COUNT,  the number of cells per liter\n",
    "- `ReportingValue` (taxonomic), `ReportedValue` (sediment), or `IBIValue` (bio mass) the value of the parameter.\n",
    "- `ReportingUnits` This parameter describes the units in which a substance is measured. \n",
    "- `ProjectIdentifier`\n",
    "- `Units` units for the sample volume. Always centimeters (cubic centimeters?)\n",
    "- `SampleVolume` Total Volume of Sample\n",
    "- `PDepth`, Composite Sample Cut Off Depth (meters)\n",
    "- `Salzone`, Salinity Zone\n",
    "     - HM, High Mesohaline =>12 TO 18 parts per thousand\n",
    "     - LM, Low Mesohaline =>5.0 TO 12 parts per thousand\n",
    "     - M, Mesohaline =>5.0 TO 18 parts per thousand\n",
    "     - O, Oligohaline =>0.5 TO 5.0 parts per thousand\n",
    "     - P, Polyhaline =>18 parts per thousand\n",
    "     - TF, Tidal Fresh < 0.5 parts per thousand\n",
    "\n",
    "Additionally, the taxonomic dataset has \n",
    "- `GMethod` Chesapeake Bay Program Gear Method Code. Codes represent information relating to the type of field gear used to collect samples for all analysis. In this dataset all are 7, Plankton Pump\n",
    "- `TSN` ITIS Taxon Serial Number, unique to the species. When used in conjunction with the NODC, the TSN\n",
    "overcomes the problem of numeric changes in the NODC code whenever species are reclassified. \n",
    "- `LatinName` Species Latin Name \n",
    "- `Size` Cell Size Groupings when taken. Some species have different measurements for different sizes. \n",
    "- `LifeStageDescription`, a numeric code of the life stage. Most are 89 - not specified.\n",
    "\n",
    "For initial cleaning, we will remove `Layer`, `Units`. Other cleaning will be done based on the dataset.\n",
    "\n",
    "Then we will turn the paramenters into columns populated by the measured values.\n",
    "\n",
    "Finally, combine all four datasets on the values for them monitoring data (other than layer and unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Individual Datasets\n",
    "\n",
    "The first step of data cleaning is working with the individual datasets and determining what information we need to keep. Some of this cleaning will be done with the aid of the DataWrangler extension in VSCode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sediment\n",
    "\n",
    "Opening `../data/plank_ChesapeakeBenthicSediment.csv` in DataWrangler shows that `SampleVolume` is missing 56 values. Let's replace '' and NaN with None and check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment = pd.read_csv('../data/plank_ChesapeakeBenthicSediment.csv')\n",
    "\n",
    "sediment_clean = sediment.replace('', np.nan).where(sediment.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4725, 19)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sediment_clean = sediment_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "sediment_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ReportingParameter` and `ReportingUnits` columns to create a dictionary, then remove the `ReportingUnits` column. We need to find the unique values in `ReportingParameter` to create a dictionary of their meanings. The information is in a PDF which cannot be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MOIST' 'SAND' 'TC' 'TIC' 'SILTCLAY' 'TOC' 'TN' 'VOLORG' 'KURTOSIS'\n",
      " 'CLAY' 'MEANDIAM' 'SORT' 'SKEWNESS']\n"
     ]
    }
   ],
   "source": [
    "sediment_parameters = sediment_clean['ReportingParameter'].unique()\n",
    "\n",
    "print(sediment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_parameters_meanings = ['Sediment Moisture Percentage', 'Sand Content, Percent', 'Total Carbon Content','Total Inorganic Carbonate Content','Silt Clay Content, Percent','Total Organic Carbon','Total Nitrogen','Volatile Organic, Percent','Kurtosis','Clay Content, Percent','Mean Sediment Diameter','Sorting','Skewness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from parameters to their meanings\n",
    "param_to_meaning = {param: meaning for param, meaning in zip(sediment_parameters, sediment_parameters_meanings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial dictionary with units and empty types\n",
    "sediment_param_dict = {param: {'Units': unit, 'Type': \"\"} for param, unit in zip(sediment_clean['ReportingParameter'], sediment_clean['ReportingUnits'])}\n",
    "\n",
    "# Update the dictionary with the meanings\n",
    "for param in sediment_param_dict:\n",
    "    if param in param_to_meaning:\n",
    "        sediment_param_dict[param]['Type'] = param_to_meaning[param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_clean = sediment_clean.drop(columns='ReportingUnits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename `ReportingParameter` as `ReportedParameter`. This will allow us to define a tranformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_clean = sediment_clean.rename(columns={'ReportingParameter':'ReportedParameter'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass = pd.read_csv('../data/plank_ChesapeakeBenthicBioMass.csv')\n",
    "\n",
    "biomass_clean = biomass.replace('', np.nan).where(biomass.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these samples were not in the monitoring data file, so `SampleVolume`, `PDepth`, and `Salzone` are mostly empty. So let's also drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26904, 15)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomass_clean = biomass_clean.drop(columns=['Layer','Units','SampleVolume','PDepth','Salzone'])\n",
    "\n",
    "biomass_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `IBIParameter` and `IBIValue` columns to create a dictionary, then remove the `IBIValue` column. We need to find the unique values in `IBIParameter` to create a dictionary of their meanings. The information is in a PDF which cannot be scraped, but ther are 126 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_parameters = biomass_clean['IBIParameter'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a few rows that do not encode information that were missed by the cleaning in the download. Let's drop the rows where `IBIParameter` is missing (from DataWrangler, these are the correct rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_clean = biomass_clean.dropna(subset=['IBIParameter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename `IBIParameter` as `ReportedParameter` and `IBIValue` as `ReportedValue`. This will allow us to define a tranformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_clean = biomass_clean.rename(columns={'IBIParameter':'ReportedParameter','IBIValue':'ReportedValue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic Counts\n",
    "\n",
    "Opening `../data/plank_ChesapeakeBenthicTaxonomic.csv` in DataWrangler shows there are also a few rows that do not encode information that were missed by the cleaning in the download. Let's drop the rows where `LatinName` is missing (from DataWrangler, these are the correct rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic = pd.read_csv('../data/plank_ChesapeakeBenthicTaxonomic.csv')\n",
    "\n",
    "taxonomic_clean = taxonomic.replace('', np.nan).where(taxonomic.notna(), None)\n",
    "\n",
    "taxonomic_clean = taxonomic_clean.dropna(subset=['LatinName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26128, 22)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomic_clean = taxonomic_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "taxonomic_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary for `LifeStageDescription`, from the table in the user guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.,  79., 248.,  76.,  97., 247., 245.,  53.,  21., 225.,  93.,\n",
       "        52., 232.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomic_clean['LifeStageDescription'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_stage_dict = {\n",
    "    89.: 'Not Specified',  \n",
    "    79.: 'Species', \n",
    "    248.: 'Immature Without Cap. Chaete',  \n",
    "    76.: 'Group',  \n",
    "    97.: 'Larvae', \n",
    "    247.: 'Immature With Cap. Chaete', \n",
    "    245.: 'Type',  \n",
    "    53.: 'Species B',  \n",
    "    21.: 'Pupae', \n",
    "    225.: 'Complex',  \n",
    "    93.: 'Juvenile',\n",
    "    52.: 'Species A', \n",
    "    232.: 'Species M'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `LatinName` and `TSN` are already categorical data, we will still with the words version. Let's replace the `LifeStateDescription` with the actual description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean['LifeStageDescription'] = taxonomic_clean['LifeStageDescription'].replace(life_stage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many `LatinNames` are measured at multiple life stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LatinName</th>\n",
       "      <th>LifeStageDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>Ampelisca</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>Ampelisca</td>\n",
       "      <td>Juvenile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Axarus</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Axarus</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>Bivalvia</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11076</th>\n",
       "      <td>Bivalvia</td>\n",
       "      <td>Species B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Larvae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Pupae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25684</th>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11332</th>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>Gastropoda</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>Gastropoda</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25546</th>\n",
       "      <td>Maldanidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>Maldanidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>Nemertea</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Nemertea</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15988</th>\n",
       "      <td>Nereidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12892</th>\n",
       "      <td>Nereidae</td>\n",
       "      <td>Species A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13813</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15754</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Species M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Trepaxonemata</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11664</th>\n",
       "      <td>Trepaxonemata</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tubificidae</td>\n",
       "      <td>Immature Without Cap. Chaete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Tubificidae</td>\n",
       "      <td>Immature With Cap. Chaete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Tubificoides</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>Tubificoides</td>\n",
       "      <td>Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LatinName          LifeStageDescription\n",
       "24636      Ampelisca                       Species\n",
       "8691       Ampelisca                      Juvenile\n",
       "46            Axarus                 Not Specified\n",
       "641           Axarus                       Species\n",
       "11432       Bivalvia                       Species\n",
       "11076       Bivalvia                     Species B\n",
       "33      Chironomidae                        Larvae\n",
       "351     Chironomidae                         Pupae\n",
       "629     Chironomidae                 Not Specified\n",
       "25684  Enchytraeidae                 Not Specified\n",
       "11332  Enchytraeidae                       Species\n",
       "9884      Gastropoda                       Species\n",
       "3403      Gastropoda                 Not Specified\n",
       "25546     Maldanidae                 Not Specified\n",
       "9470      Maldanidae                       Species\n",
       "8723        Nemertea                       Species\n",
       "41          Nemertea                 Not Specified\n",
       "15988       Nereidae                       Species\n",
       "12892       Nereidae                     Species A\n",
       "22       Oligochaeta                 Not Specified\n",
       "13813    Oligochaeta                       Species\n",
       "15754    Oligochaeta                     Species M\n",
       "192    Trepaxonemata                 Not Specified\n",
       "11664  Trepaxonemata                       Species\n",
       "3        Tubificidae  Immature Without Cap. Chaete\n",
       "168      Tubificidae     Immature With Cap. Chaete\n",
       "312     Tubificoides                       Species\n",
       "8701    Tubificoides                         Group"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by LatinName and check the unique values in LifeStageDescriptions\n",
    "groups = taxonomic_clean.groupby('LatinName')['LifeStageDescription'].nunique()\n",
    "\n",
    "# Identify LatinName values where LifeStageDescriptions has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['LatinName'].isin(disagreeing_a_values)]\n",
    "\n",
    "filtered_df[['LatinName','LifeStageDescription']].drop_duplicates().sort_values(by='LatinName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the `LatinName` and `LifeStageDescription` columns. We will drop `Not Specified`. It appears that in this dataset, any `LatinName` has at most one `LifeStageDescription` that contains `Species`, but we will keep those values to be safe, since some have 'Not Specified'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean['LatinName'] = taxonomic_clean['LatinName'] + ' ' +taxonomic_clean['LifeStageDescription'].replace('Not Specified', '')\n",
    "\n",
    "taxonomic_clean= taxonomic_clean.drop(columns='LifeStageDescription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are `FieldActivityID` and `EventId` different numbering systems for the same thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FieldActivityId</th>\n",
       "      <th>EventId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FieldActivityId, EventId]\n",
       "Index: []"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by FieldActivityId and check the unique values in EventId\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['EventId'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['FieldActivityId'].isin(disagreeing_a_values)]\n",
    "\n",
    "filtered_df[['FieldActivityId','EventId']].drop_duplicates().sort_values(by='FieldActivityId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are! Do they include depth? time? GMethod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalDepth (0, 2)\n",
      "PDepth (0, 2)\n",
      "SampleTime (0, 2)\n",
      "GMethod (0, 2)\n"
     ]
    }
   ],
   "source": [
    "# Group by FieldActivityId and check the unique values in TotalDepth\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['TotalDepth'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where TotalDepth has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['FieldActivityId'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('TotalDepth',filtered_df[['FieldActivityId','TotalDepth']].drop_duplicates().sort_values(by='FieldActivityId').shape)\n",
    "\n",
    "# Group by FieldActivityId and check the unique values in PDepth\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['PDepth'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where TotalDepth has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['FieldActivityId'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('PDepth',filtered_df[['FieldActivityId','PDepth']].drop_duplicates().sort_values(by='FieldActivityId').shape)\n",
    "\n",
    "# Group by FieldActivityId and check the unique values in SampleTime\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['SampleTime'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['SampleTime'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('SampleTime',filtered_df[['FieldActivityId','SampleTime']].drop_duplicates().sort_values(by='SampleTime').shape)\n",
    "\n",
    "# Group by FieldActivityId and check the unique values in GMethod\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['GMethod'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['GMethod'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('GMethod',filtered_df[['FieldActivityId','GMethod']].drop_duplicates().sort_values(by='GMethod').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `TSN` always the same for `LatinName`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.drop_duplicates of Empty DataFrame\n",
       "Columns: [CBSeg2003, CBSeg2003Description, Station, Latitude, Longitude, SampleType, FieldActivityId, SampleDate, SampleTime, TotalDepth, ReportingValue, ReportingUnit, EventId, Source, GMethod, TSN, LatinName, ProjectIdentifier, Salzone, PDepth, SampleVolume]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by LatinName and check the unique values in GMethod\n",
    "groups = taxonomic_clean.groupby('LatinName')['TSN'].nunique()\n",
    "\n",
    "# Identify LatinName values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "taxonomic_clean[taxonomic_clean['TSN'].isin(disagreeing_a_values)].drop_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pivot the `LatinName` column and `ReportingValue` column. \n",
    "We drop `TSN` since we are using `LatinName` (the other option would be to also pivot on `TSN` and `ReportingValue`). Since `GMethod` is information about how the sample was collected, and no needed for merging datasets, we will also remove it. \n",
    "\n",
    "The `ReportingUnit` is always `CM`, so we can also drop it. Also, some of these measurements must be counts, unless some clams are over a kilometer in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean = taxonomic_clean.drop(columns=['TSN','GMethod','ReportingUnit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with ReportingValues column -- not used later\n",
    "\n",
    "Since some values in the `ReportingValue` column appear to be counts and some are size, let's see if there are any rows that agree in all columns except `ReportingValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude from comparison\n",
    "exclude_columns = ['ReportingValue']\n",
    "\n",
    "# Columns to compare\n",
    "compare_columns = [col for col in taxonomic_clean.columns if col not in exclude_columns]\n",
    "\n",
    "# Group by the columns to compare\n",
    "grouped = taxonomic_clean.groupby(compare_columns)\n",
    "\n",
    "# Initialize lists to collect rows based on group size\n",
    "groups_with_2_rows = []\n",
    "groups_with_3_rows = []\n",
    "groups_with_more_than_3_rows = []\n",
    "\n",
    "# Iterate through each group\n",
    "for group_key, group in grouped:\n",
    "    num_rows = len(group)\n",
    "    \n",
    "    # Separate groups based on number of rows\n",
    "    if num_rows == 2:\n",
    "        groups_with_2_rows.append(group)\n",
    "    elif num_rows == 3:\n",
    "        groups_with_3_rows.append(group)\n",
    "    elif num_rows > 3:\n",
    "        groups_with_more_than_3_rows.append(group)\n",
    "\n",
    "# Create DataFrames for viewing\n",
    "df_groups_with_2_rows = pd.concat(groups_with_2_rows).sort_values(by=['LatinName', 'EventId']) if groups_with_2_rows else pd.DataFrame()\n",
    "\n",
    "df_groups_with_3_rows = pd.concat(groups_with_3_rows).sort_values(by=['LatinName', 'EventId','ReportingValue']) if groups_with_3_rows else pd.DataFrame()\n",
    "\n",
    "df_groups_with_more_than_3_rows = pd.concat(groups_with_more_than_3_rows).sort_values(by=['LatinName', 'EventId','ReportingValue']).drop_duplicates() if groups_with_more_than_3_rows else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure what to do about the groups with more than two rows, so let's continue with the groups with two rows and come back. For the groups with two rows, we will check if one of the values is an integer. If it is, then that will be the count and the other value will be the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to accumulate results\n",
    "results_list = []\n",
    "error_groups_list = []\n",
    "\n",
    "# Iterate over each group in the existing 'grouped' object\n",
    "for group_key, group in grouped:\n",
    "    if len(group) == 2:\n",
    "        # Extract values from the group\n",
    "        values = group['ReportingValue'].values\n",
    "        \n",
    "        # Check which values are integers\n",
    "        int_values = [v for v in values if v == round(v)]\n",
    "        \n",
    "        if len(int_values) == 1:\n",
    "            # One integer value found\n",
    "            reporting_count = int_values[0]\n",
    "            reporting_size = values[0] if values[0] != reporting_count else values[1]\n",
    "            \n",
    "            # Initialize new columns\n",
    "            group = group.copy()\n",
    "            group['ReportingCount'] = reporting_count\n",
    "            group['ReportingSize'] = reporting_size\n",
    "            \n",
    "            # Append to results list\n",
    "            results_list.append(group)\n",
    "        else:\n",
    "            # Append to error_groups list\n",
    "            error_groups_list.append(group)\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "taxonomic_two_measures = pd.concat(results_list, ignore_index=True) if results_list else pd.DataFrame(columns=taxonomic_clean.columns.tolist() + ['ReportingCount', 'ReportingSize'])\n",
    "\n",
    "error_groups_df = pd.concat(error_groups_list, ignore_index=True) if error_groups_list else pd.DataFrame(columns=taxonomic_clean.columns)\n",
    "\n",
    "# Sort the results DataFrame by 'LatinName' and 'EventId'\n",
    "taxonomic_two_measures.sort_values(by=['LatinName', 'EventId'], inplace=True)\n",
    "\n",
    "# Sort the error groups DataFrame by 'LatinName' and 'EventId'\n",
    "error_groups_df.sort_values(by=['LatinName', 'EventId'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error group contains four rows where every value is an integer. We will handle that case and the groups of size 1 together. Let's generate the groups of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to accumulate single element groups\n",
    "single_element_groups_list = []\n",
    "\n",
    "# Iterate over each group in the existing 'grouped' object\n",
    "for group_key, group in grouped:\n",
    "    if len(group) == 1:\n",
    "        # Append single element groups to single_element_groups_list\n",
    "        single_element_groups_list.append(group)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "taxonomic_one_measure = pd.concat(single_element_groups_list, ignore_index=True) if single_element_groups_list else pd.DataFrame(columns=taxonomic_clean.columns)\n",
    "\n",
    "# Optionally, sort the DataFrame by 'LatinName'\n",
    "taxonomic_one_measure.sort_values(by=['LatinName'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since we will want to update this later, let's make a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranges_dict(dataframe, column_to_fix, ranges_columns):\n",
    "    dictionary = {}\n",
    "    for latin_name, group in dataframe.groupby(column_to_fix):\n",
    "        col_dict = {}\n",
    "        for col in ranges_columns:\n",
    "            if col in group.columns:\n",
    "                # Get the min and max values for each column\n",
    "                reporting_range = [group[col].min(), group[col].max()]\n",
    "                # Add the range to the column dictionary\n",
    "                col_dict[col + 'Range'] = reporting_range\n",
    "        # Store the column dictionary in the main dictionary\n",
    "        dictionary[latin_name] = col_dict\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges_dict = create_ranges_dict(taxonomic_two_measures,'LatinName',['ReportingSize','ReportingCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_in_ranges(row):\n",
    "    latin_name = row['LatinName']\n",
    "    reporting_value = row['ReportingValue']\n",
    "    \n",
    "    if latin_name in ranges_dict:\n",
    "        count_range = ranges_dict[latin_name]['ReportingCountRange']\n",
    "        size_range = ranges_dict[latin_name]['ReportingSizeRange']\n",
    "        \n",
    "        # Check if the reporting_value is within either range\n",
    "        in_count_range = count_range[0] <= reporting_value <= count_range[1]\n",
    "        in_size_range = size_range[0] <= reporting_value <= size_range[1]\n",
    "        \n",
    "        if in_count_range and in_size_range:\n",
    "            return 'Both'\n",
    "        elif in_count_range:\n",
    "            # check if integer\n",
    "            if reporting_value == round(reporting_value):\n",
    "                return 'In Count Range'\n",
    "            else:\n",
    "                return 'In Count Range, not integer'\n",
    "    \n",
    "        elif in_size_range:\n",
    "            return 'In Size Range'\n",
    "        else:\n",
    "            # check if integer\n",
    "            if reporting_value == round(reporting_value):\n",
    "                return 'Not in ranges, integer'\n",
    "            else:\n",
    "                return 'Not in ranges, not integer'\n",
    "    else:\n",
    "        # check if integer\n",
    "        if reporting_value == round(reporting_value):\n",
    "            return 'Taxa not in dictionary, integer'\n",
    "        else:\n",
    "            return 'Taxa not in dictionary, not integer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if the `ReportedValue` is in the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_one_measure['InRange'] = taxonomic_one_measure.apply(check_value_in_ranges, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add some of these values to the correct column to start creating our new dataset. We will remove those rows from `taxonomic_one_measure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows that can be updayed\n",
    "condition = taxonomic_one_measure['InRange'].isin(['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer']) | (taxonomic_one_measure['InRange'] == 'In Count Range')\n",
    "\n",
    "# Create the new DataFrame with all columns and additional columns 'ReportingSize' and 'ReportingCount'\n",
    "new_taxonomic_one_measure = taxonomic_one_measure[condition].copy()\n",
    "new_taxonomic_one_measure['ReportingSize'] = new_taxonomic_one_measure.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] in ['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer'] else None, \n",
    "    axis=1\n",
    ")\n",
    "new_taxonomic_one_measure['ReportingCount'] = new_taxonomic_one_measure.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] == 'In Count Range' else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove the identified rows from the original DataFrame while retaining all columns\n",
    "# Drop InRanges to rerun\n",
    "taxonomic_one_measure = taxonomic_one_measure.loc[~condition].drop(columns='InRange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same thing with the error group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add InRanges\n",
    "error_groups_df['InRange'] = error_groups_df.apply(check_value_in_ranges, axis=1)\n",
    "\n",
    "# Move the values we can\n",
    "# Identify rows that can be updayed\n",
    "condition = error_groups_df['InRange'].isin(['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer']) | (error_groups_df['InRange'] == 'In Count Range')\n",
    "\n",
    "# Create the new DataFrame with all columns and additional columns 'ReportingSize' and 'ReportingCount'\n",
    "new_error_groups_df = error_groups_df[condition].copy()\n",
    "new_error_groups_df['ReportingSize'] = new_error_groups_df.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] in ['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer'] else None, \n",
    "    axis=1\n",
    ")\n",
    "new_error_groups_df['ReportingCount'] = new_error_groups_df.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] == 'In Count Range' else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove the identified rows from the original DataFrame while retaining all columns\n",
    "# Drop InRanges to rerun\n",
    "error_groups_df = error_groups_df.loc[~condition].drop(columns='InRange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine our dataframes, update the dictionary, and run ranges assessment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_94891/2626544837.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_taxonomic_data = pd.concat([new_error_groups_df,new_taxonomic_one_measure,taxonomic_two_measures],ignore_index=True).drop(columns=['ReportingValue','InRange']).drop_duplicates()\n"
     ]
    }
   ],
   "source": [
    "combined_taxonomic_data = pd.concat([new_error_groups_df,new_taxonomic_one_measure,taxonomic_two_measures],ignore_index=True).drop(columns=['ReportingValue','InRange']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate our dictionary and try again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Quality\n",
    "\n",
    "Opening `../data/plank_ChesapeakeBenthicWaterQuality.csv` in DataWrangler shows that `SampleVolume` is missing 94 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = pd.read_csv('../data/plank_ChesapeakeBenthicWaterQuality.csv')\n",
    "\n",
    "water_clean = water.replace('', np.nan).where(water.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8427, 21)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_clean = water_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "water_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ReportedParameter` and `ReportingUnits` columns to create a dictionary, then remove the `ReportingUnits` column. We need to find the unique values in `ReportingParameter` to create a dictionary of their meanings. The information is in a PDF which cannot be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PH' 'WTEMP' 'DO' 'DO_SAT_P' 'SALINITY' 'SPCOND']\n"
     ]
    }
   ],
   "source": [
    "water_parameters = water_clean['ReportedParameter'].unique()\n",
    "\n",
    "print(water_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for the parameters. All are method F01, in-situ measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial dictionary with units and empty types\n",
    "water_param_dict = {'PH':'pH', 'WTEMP' : 'Water Temperature', 'DO': 'Dissolved Oxygen', 'DO_SAT_P': 'Dissolved oxygen relative to theoretical value at saturation (%)', 'SALINITY': 'Salinity in-situ measured with probe', 'SPCOND' :'Specific Conductance At 25 C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_clean = water_clean.drop(columns=['ReportedUnits','WQMethod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn parameters into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_to_columns(dataframe,columns_to_group):\n",
    "    # Reset index to use row numbers as the index\n",
    "    df_reset = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Pivot the DataFrame while preserving non-pivoted columns\n",
    "    df_pivoted = df_reset.pivot_table(index=df_reset.index, columns='ReportedParameter', values='ReportedValue', aggfunc='first')\n",
    "\n",
    "    # Combine pivoted result with the original DataFrame columns not involved in the pivot\n",
    "    df_pivoted = df_reset.drop(columns=['ReportedParameter','ReportedValue']).join(df_pivoted)\n",
    "\n",
    "    #check unique combinations\n",
    "    #this allows us to check that we havent lost data\n",
    "    unique_combinations = df_pivoted[columns_to_group].drop_duplicates()\n",
    "\n",
    "    # Check we haven't lost unique non-empty values\n",
    "    for col in columns_to_group:\n",
    "        # Filter out empty values\n",
    "        df_combined_nonempty = df_pivoted[col].dropna()\n",
    "        df_pivoted_nonempty = dataframe[col].dropna()\n",
    "        \n",
    "        # Check if the number of unique non-empty values matches\n",
    "        unique_check = df_combined_nonempty.unique().size == df_pivoted_nonempty.unique().size\n",
    "        \n",
    "        # Print the results\n",
    "        print(\"Checking unique values in \" ,col, unique_check)\n",
    "    \n",
    "    # Create a copy of the DataFrame for processing\n",
    "    df_processed = df_pivoted.copy()\n",
    "\n",
    "    # Create a unique identifier for each group based on the columns to match\n",
    "    df_processed['UniqueID'] = df_processed[columns_to_group].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "    # Group by the unique identifier\n",
    "    df_combined = df_processed.groupby('UniqueID', as_index=False).first()\n",
    "\n",
    "    # Drop the UniqueID column and remove duplicates\n",
    "    df_really_clean = df_combined.drop(columns='UniqueID').drop_duplicates()\n",
    "\n",
    "    return df_really_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns to group come from the monitoring data, so should be present in all datasets. This is an issue for the BioMass dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_group = [\"CBSeg2003\",\"CBSeg2003Description\",\"ProjectIdentifier\",\"FieldActivityId\",\"Source\",\"Station\",\"SampleDate\",\"Latitude\",\"Longitude\",\"PDepth\",\"Salzone\",\"SampleVolume\",\"TotalDepth\",\"SampleTime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  SampleVolume True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  SampleTime True\n"
     ]
    }
   ],
   "source": [
    "sediment_really_clean = parameter_to_columns(sediment_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bio Mass needs a different list of columns, since we removed some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_columns_to_group = [\"CBSeg2003\",\"CBSeg2003Description\",\"ProjectIdentifier\",\"FieldActivityId\",\"Source\",\"Station\",\"SampleDate\",\"Latitude\",\"Longitude\",\"TotalDepth\",\"SampleTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  SampleTime True\n"
     ]
    }
   ],
   "source": [
    "biomass_really_clean = parameter_to_columns(biomass_clean,biomass_columns_to_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some final cleaning for Taxonomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean = taxonomic_clean.rename(columns={'ReportingValue': 'ReportedValue','LatinName':'ReportedParameter'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  SampleVolume True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  SampleTime True\n"
     ]
    }
   ],
   "source": [
    "taxonomic_really_clean = parameter_to_columns(taxonomic_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  SampleVolume True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  SampleTime True\n"
     ]
    }
   ],
   "source": [
    "water_really_clean = parameter_to_columns(water_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Dataset\n",
    "\n",
    "Now we combine on 'CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
    "       'EventId', 'Source', 'SampleDate', 'SampleDepth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sediment columns: Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'FieldActivityId', 'SampleDate', 'SampleTime', 'TotalDepth', 'EventId',\n",
      "       'Source', 'SampleReplicate', 'ReportedParameter', 'ReportedValue',\n",
      "       'ProjectIdentifier', 'SampleVolume', 'PDepth', 'Salzone'],\n",
      "      dtype='object')\n",
      "BioMass columns: Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'FieldActivityId', 'SampleDate', 'SampleTime', 'TotalDepth',\n",
      "       'BiologicalEventId', 'Source', 'SampleReplicate', 'ReportedParameter',\n",
      "       'ReportedValue', 'ProjectIdentifier'],\n",
      "      dtype='object')\n",
      "Taxonomic columns: Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'SampleType', 'FieldActivityId', 'SampleDate', 'SampleTime',\n",
      "       'TotalDepth', 'ReportedValue', 'EventId', 'Source', 'ReportedParameter',\n",
      "       'ProjectIdentifier', 'Salzone', 'PDepth', 'SampleVolume'],\n",
      "      dtype='object')\n",
      "Water columns Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
      "       'FieldActivityId', 'SampleDate', 'SampleTime', 'TotalDepth', 'EventId',\n",
      "       'Source', 'SampleDepth', 'SampleReplicate', 'ReportedParameter',\n",
      "       'ReportedValue', 'ProjectIdentifier', 'SampleVolume', 'PDepth',\n",
      "       'Salzone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Use clean to have fewer columns\n",
    "print('Sediment columns:', sediment_clean.columns)\n",
    "print('BioMass columns:', biomass_clean.columns)\n",
    "print('Taxonomic columns:', taxonomic_clean.columns)\n",
    "print('Water columns', water_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of some extra columns before merging. `BiologicalEventId` is only in BioMass, `EventId` is simply a different system for recoding the same information as `FieldActivityId`. `SampleReplicate` should not matter, but migh prevent some merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_really_clean = biomass_really_clean.drop(columns=['SampleReplicate','BiologicalEventId'])\n",
    "taxonomic_really_clean = taxonomic_really_clean.drop(columns=['EventId'])\n",
    "water_really_clean = water_really_clean.drop(columns=['EventId','SampleReplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sediment_really_clean and biomass_really_clean\n",
    "merged_df = pd.merge(sediment_really_clean, biomass_really_clean, how='outer', on=[col for col in sediment_really_clean.columns if col in biomass_really_clean.columns], suffixes=('', '_biomass_really_clean'))\n",
    "\n",
    "# Merge the result with taxonomic_really_clean\n",
    "merged_df = pd.merge(merged_df, taxonomic_really_clean, how='outer', on=[col for col in merged_df.columns if col in taxonomic_really_clean.columns and not col.endswith('_biomass_really_clean')], suffixes=('', '_taxonomic_really_clean'))\n",
    "\n",
    "# Merge the result with water_really_clean\n",
    "merged_df = pd.merge(merged_df, water_really_clean, how='outer', on=[col for col in merged_df.columns if col in water_really_clean.columns and not col.endswith(('_biomass_really_clean', '_taxonomic_really_clean'))], suffixes=('', '_water_really_clean'))\n",
    "\n",
    "# Reset the index for better readability\n",
    "merged_df = merged_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some issues with different precisions for latitude and longitude causing lack of matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define columns for matching\n",
    "match_columns = [\n",
    "    'CBSeg2003', 'CBSeg2003Description', 'Station', 'FieldActivityId', 'SampleDate',\n",
    "    'SampleTime', 'TotalDepth', 'Source', 'ProjectIdentifier'\n",
    "]\n",
    "\n",
    "# Generate a composite key based on the matching columns\n",
    "merged_df['unique_key'] = merged_df[match_columns].apply(lambda row: tuple(row.fillna('missing')), axis=1)\n",
    "\n",
    "# Handle Latitude and Longitude with precision\n",
    "# Keep the most precise value for Latitude and Longitude\n",
    "merged_df['Latitude'] = merged_df.groupby('unique_key')['Latitude'].transform(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "merged_df['Longitude'] = merged_df.groupby('unique_key')['Longitude'].transform(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "\n",
    "# Aggregate the groups\n",
    "result_df = merged_df.groupby('unique_key').first().reset_index()\n",
    "\n",
    "# Drop the unique_key column from the result\n",
    "result_df = result_df.drop(columns=['unique_key'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('../data/plank_ChesapeakeBayBenthic_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
