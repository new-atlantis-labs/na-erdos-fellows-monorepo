{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "makefile"
    }
   },
   "source": [
    "# Introduction \n",
    "\n",
    "The Chesapeake Bay Program [DataHub](https://datahub.chesapeakebay.net/Home) contains many datasets for the Chesapeake Bay. \n",
    "\n",
    "The Water Quality Data is still updating and measures many field and lab parameters including: phosphorus, nitrogen, carbon, various other lab parameters (suspended solids, disolved solids, chlorophyll-a, alkalinkity, etc), dissolved oxygen, pH, salinity, turbitity, water temperature, and climate condition. See [Guide to Using Chesapeake Bay Program Water Quality Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/wq_data_userguide_10feb12_mod.pdf) for more information.\n",
    "\n",
    "The Living Resources database includes biological monitoring data from the Chesapeake Bay Program. From the [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf):\n",
    ">All Chesapeake Bay phytoplankton, historic zooplankton (including microzooplankton, mesozooplankton and gelatinous zooplankton) and benthos monitoring data and data documentation for Maryland and Virginia from 1984 to present can be obtained directly from the ... Living Resources Data Manager.\n",
    "\n",
    "There is also a [DataHub API](https://datahub.chesapeakebay.net/API) which we will use to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports we need in the notebook. This allows sections or subsections to be run without needing to repeat import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic restriction\n",
    "\n",
    "The Chesapeake Bay segements are based on circulation and salinity properties of different areas of the Bay.\n",
    "Since we want the same geographic information for every dataset, let's go ahead and generate that now. We need to retrieve the `Geographical-Id`s from the [Geographical-Attribute, CBSeg2003 list](https://datahub.chesapeakebay.net/api.json/CBSeg2003). Since we only want the segments in the Bay proper, we will search for the segment names that start with `CB`. We will then add the sounds and bays that adjoin the Chesapeake Bay. We will not include the segments for bays on the other side of the Eastern Shore from the Chesapeake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003/10,11,12,13,14,15,16,17,28,49,84/\n"
     ]
    }
   ],
   "source": [
    "# Add CBSeg2003 to idValues to specify type of geographic ID\n",
    "GeographicID_values = 'CBSeg2003/'\n",
    "\n",
    "# Define the URL with the CBSeg2003 list\n",
    "CBSeg2003_url = \"http://datahub.chesapeakebay.net/api.json/CBSeg2003\"\n",
    "\n",
    "# Send a GET request to the list\n",
    "response = requests.get(CBSeg2003_url)\n",
    "filtered_segments=[]\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Filter the results to find CBSeg2003Name that start with \"CB\"\n",
    "        filtered_segments = [\n",
    "            segment['CBSeg2003Id'] for segment in data\n",
    "            if segment.get('CBSeg2003Name', '').startswith('CB') or \n",
    "                # Add the aadjacent bays and Tangier sound\n",
    "               segment.get('CBSeg2003Name', '') in ['EASMH ', 'MOBPH ', 'TANMH ']\n",
    "        ]\n",
    "\n",
    "        # Append ids to idValues\n",
    "        if filtered_segments:\n",
    "            GeographicID_values += ','.join(map(str, filtered_segments)) + '/'\n",
    "        else:\n",
    "            print(\"No matching segments found.\")\n",
    "        \n",
    "        print(GeographicID_values)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to parse JSON data: {e}\")\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living Resources\n",
    "\n",
    "The Living Resources database has three databases and we must access each separately. The [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf) \n",
    "\n",
    "This document is out of date for the Plankton Database, but the Plankton Database does not require merging except by project. The Tidal Benthic Database requires merging by data type, as well. We will use the monitoring event files by project before me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's write a function to fetch all relevant project ids and create a dictionary. This function takes in a list of identifiers (as strings) and parses the relevent url. The default API url is the Living Resources Projects list JSON.\n",
    "\n",
    "Note that `ProjectIdentifier` is the abbreviation for the project name, while `ProjectId` is the number we need for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the project IDs\n",
    "def get_project_dict(projectIdentifiers,url= \"https://datahub.chesapeakebay.net/api.json/LivingResources/Projects\"):\n",
    "\n",
    "    # Send a GET request to the list\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            # Parse the JSON response\n",
    "            data = response.json()\n",
    "            \n",
    "            # Filter the results to find projects with ProjectIdentifier in projectIdentifiers\n",
    "            filtered_projects = [\n",
    "                project for project in data \n",
    "                if project.get('ProjectIdentifier') in projectIdentifiers\n",
    "            ]\n",
    "            \n",
    "            # Extract the ProjectName and ProjectId for the filtered results\n",
    "            project_info = {\n",
    "                project['ProjectId']: {\n",
    "                     project['ProjectIdentifier'] : project['ProjectName']\n",
    "                } for project in filtered_projects\n",
    "            }\n",
    "            \n",
    "            return project_info\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Failed to parse JSON data: {e}\")\n",
    "            return {}\n",
    "    else:\n",
    "        # Handle the error\n",
    "        print(f\"Failed to retrieve data from {url}: {response.status_code} - {response.text}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function to get the monitoring event data set for each project. The data will be stored in a dictionary with the ProjectIdentifier as the key.\n",
    "\n",
    "This function will take in the same list of identifiers as `get_project_dict`, a start date in MM-DD-YYYY form, an end date in MM-DD-YYYY form, and the geographic attribute list. The default API url is the Living Resources csv file, but the function works for other monitoring events. The default geographic identifier is `GeographicID_values`\n",
    "\n",
    "The general form for the API url is: `http://datahub.chesapeakebay.net/api.csv/LivingResources/<Source>/MonitorEvent/<Start-Date>/<End-Date>/<Project-Id>/<Geographical-Attribute>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the monitor event data\n",
    "def fetch_monitor_data_by_project(source,projectIdentifiers,start_date,end_date,base_url=\"http://datahub.chesapeakebay.net/api.csv/LivingResources/\",geograhic_id=GeographicID_values):\n",
    "    # get project-id list\n",
    "    projects = get_project_dict(projectIdentifiers)\n",
    "\n",
    "    # Format the dates\n",
    "    start_str = start_date.strftime('%m-%d-%Y')\n",
    "    end_str = end_date.strftime('%m-%d-%Y')\n",
    "\n",
    "    # Dictionary to store DataFrames for each project\n",
    "    project_dataframes = {}\n",
    "\n",
    "    # create API url for each project\n",
    "    # create dataframe for each project\n",
    "    for project_id, info in projects.items():\n",
    "        project_abr, project_name = next(iter(info.items()))\n",
    "\n",
    "        api_url=f\"{base_url}{source}/MonitorEvent/{start_str}/{end_str}/{project_id}/{geograhic_id}\"\n",
    "\n",
    "        # Fetch data from the URL, skipping totals row\n",
    "        df = pd.read_csv(api_url, skipfooter=1, engine='python')\n",
    "\n",
    "        # Add ProjectIdentifier column if it does not exist\n",
    "        if 'ProjectIdentifier' not in df.columns:\n",
    "            df['ProjectIdentifier'] = project_abr\n",
    "\n",
    "        # Store the DataFrame in the dictionary using the project abbreviation as the key\n",
    "        project_dataframes[project_abr] = df\n",
    "\n",
    "    return project_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need functions to download the project data and combine with the monitor data. The later is a bit trickier for the Tidal Benthic database, as it does not use consistent column names for the different datasets.\n",
    "\n",
    "The general form for the API url is: `http://datahub.chesapeakebay.net/api.csv/LivingResources/<Source>/<Data-Type>/<Start-Date>/<End-Date>/<Project-Id>/<Geographical-Attribute>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the data record\n",
    "def fetch_recorded_data_by_project(source,data_type,projectIdentifiers,start_date,end_date,base_url=\"http://datahub.chesapeakebay.net/api.csv/LivingResources/\",geograhic_id=GeographicID_values):\n",
    "    # get project-id list\n",
    "    projects = get_project_dict(projectIdentifiers)\n",
    "\n",
    "    # Format the dates\n",
    "    start_str = start_date.strftime('%m-%d-%Y')\n",
    "    end_str = end_date.strftime('%m-%d-%Y')\n",
    "\n",
    "    # Dictionary to store DataFrames for each project\n",
    "    project_dataframes = {}\n",
    "\n",
    "    # create API url for each project\n",
    "    # create dataframe for each project\n",
    "    for project_id, info in projects.items():\n",
    "        project_abr, project_name = next(iter(info.items()))\n",
    "\n",
    "        api_url=f\"{base_url}{source}/{data_type}/{start_str}/{end_str}/{project_id}/{geograhic_id}\"\n",
    "\n",
    "        # Fetch data from the URL, skipping totals row\n",
    "        df = pd.read_csv(api_url, skipfooter=1, engine='python')\n",
    "\n",
    "        # Add ProjectIdentifier column if it does not exist\n",
    "        if 'ProjectIdentifier' not in df.columns:\n",
    "            df['ProjectIdentifier'] = project_abr\n",
    "\n",
    "        # Store the DataFrame in the dictionary using the project abbreviation as the key\n",
    "        project_dataframes[source + project_abr] = df\n",
    "\n",
    "    return project_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will handle the column naming discrepencies by renaming columns in the data records. The function below will:\n",
    "- Use the dictionaries of monitor event data and data records to combine dataframes from the same project\n",
    "- The monitor event data will serve as a dictionary, where the keys are the values of the columns that are in both dataframes. This dictionary will be used to create any columns that exist in the monitor event data and not the data records\n",
    "- Merge the dataframes for each project into one dataframe\n",
    "- Save a csv of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_from_dict(dictionary, columns_to_remove):\n",
    "\n",
    "    # Iterate over the dictionary\n",
    "    for key, df in dictionary.items():\n",
    "        # Drop specified columns\n",
    "        if all(col in df.columns for col in columns_to_remove):\n",
    "            df.drop(columns=columns_to_remove, inplace=True)\n",
    "        else:\n",
    "            print(f\"Some columns to remove were not found in DataFrame for key: {key}\")\n",
    "            missing_cols = [col for col in columns_to_remove if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save_data(monitor_event_data, data_records, output_csv_path):\n",
    "    # Dictionary to store merged DataFrames for each project\n",
    "    merged_dataframes = {}\n",
    "\n",
    "    for monitor_key in monitor_event_data.keys():\n",
    "        # Find the corresponding key in data_records that contains the monitor_key as a substring\n",
    "        # data_records keys also include data source\n",
    "        data_record_key = next((key for key in data_records.keys() if monitor_key in key), None)\n",
    "        \n",
    "        if data_record_key is None:\n",
    "            print(f\"No matching data record found for monitor key: {monitor_key}\")\n",
    "            continue\n",
    "\n",
    "        # Get the corresponding dataframes for the project\n",
    "        monitor_df = monitor_event_data[monitor_key]\n",
    "        data_record_df = data_records[data_record_key]\n",
    "\n",
    "        # Find common columns to merge on\n",
    "        common_columns = list(set(monitor_df.columns).intersection(set(data_record_df.columns)))\n",
    "        \n",
    "\n",
    "        # Find \"missing\" columns\n",
    "        missing_columns = list(set(monitor_df.columns) - set(data_record_df.columns))\n",
    "\n",
    "\n",
    "        # Ensure common columns have the same data type\n",
    "        for col in common_columns:\n",
    "            if monitor_df[col].dtype != data_record_df[col].dtype:\n",
    "                target_dtype = monitor_df[col].dtype\n",
    "                print(data_record_key,col, \"type converted\")\n",
    "                try:\n",
    "                    data_record_df[col] = data_record_df[col].astype(target_dtype)\n",
    "                except ValueError:\n",
    "                    monitor_df[col] = monitor_df[col].astype(str)\n",
    "                    data_record_df[col] = data_record_df[col].astype(str)\n",
    "\n",
    "\n",
    "        # Adding missing columns to data_record_df with default values\n",
    "        for col in missing_columns:\n",
    "            data_record_df[col] = ''\n",
    "\n",
    "        # Group monitor_df by key columns\n",
    "        grouped = monitor_df.groupby(common_columns)\n",
    "\n",
    "                # Apply lambda to convert each group into a list of dictionaries\n",
    "        monitor_dict = grouped.apply(lambda x: x[missing_columns].to_dict('records'),include_groups=False).to_dict()\n",
    "\n",
    "        # Iterate over rows in data_record_df\n",
    "        for index, row in data_record_df.iterrows():\n",
    "            # Create a key tuple from the row's key columns\n",
    "            key = tuple(row[col] for col in common_columns)\n",
    "            if key in monitor_dict:\n",
    "                # Iterate over records corresponding to the key\n",
    "                for record in monitor_dict[key]:\n",
    "                    # Update data_record_df with values from the record\n",
    "                    for col in missing_columns:\n",
    "                        data_record_df.at[index, col] = record[col]\n",
    "\n",
    "        # Store the merged dataframe in the dictionary\n",
    "        merged_dataframes[data_record_key] = data_record_df\n",
    "\n",
    "    # Combine all merged dataframes into one\n",
    "    combined_df = pd.concat(merged_dataframes.values(), ignore_index=True)\n",
    "\n",
    "    # Print the shape of the combined dataframe\n",
    "    print(\"Shape of combined_df:\", combined_df.shape)\n",
    "\n",
    "    #Reorder the columns\n",
    "    desired_order=['CBSeg2003','CBSeg2003Description','Station','Latitude','Longitude','SampleType','FieldActivityId','SampleDate','SampleTime','Layer','TotalDepth','Parameter','ReportingValue','ReportingUnit']\n",
    "\n",
    "    # Filter out columns in desired_order that do not exist in the DataFrame\n",
    "    valid_order = [col for col in desired_order if col in combined_df.columns]\n",
    "\n",
    "    # Get the columns that are not in the desired_order\n",
    "    remaining_columns = [col for col in combined_df.columns if col not in valid_order]\n",
    "\n",
    "    # Combine valid_order with remaining_columns to maintain the desired order\n",
    "    final_order = valid_order + remaining_columns\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    combined_df = combined_df[final_order]\n",
    "\n",
    "    # Save the combined dataframe to a CSV file\n",
    "    combined_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plankton\n",
    "\n",
    "As mentioned above, the plankton database is much smaller than suggested by the Users Guide. Let's look at the monitoring event data for the relevent regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2004, 1, 1)\n",
    "end_date = datetime(2024, 8, 3)\n",
    "projectList_TidalPlankton = ['MEZ','MIZ','PHYTP','PICOP']\n",
    "plankton_monitor_events_dict = fetch_monitor_data_by_project(\"TidalPlankton\",projectList_TidalPlankton,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_88159/3772892355.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  plankton_combined_monitor_events = pd.concat(plankton_monitor_events_dict, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4738, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plankton_combined_monitor_events = pd.concat(plankton_monitor_events_dict, ignore_index=True)\n",
    "\n",
    "plankton_combined_monitor_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in Plankton data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton_records_dict = fetch_recorded_data_by_project(\"TidalPlankton\",\"Reported\",projectList_TidalPlankton,start_date,end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a problem because `CBSeg2003` and `CBSeg2003Description` columns exist, but their values are all missing. We need to remove these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton_records_dict = remove_columns_from_dict(plankton_records_dict,['CBSeg2003','CBSeg2003Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_88159/485952800.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(merged_dataframes.values(), ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined_df: (93467, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSeg2003</th>\n",
       "      <th>CBSeg2003Description</th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>SampleType</th>\n",
       "      <th>FieldActivityId</th>\n",
       "      <th>SampleDate</th>\n",
       "      <th>SampleTime</th>\n",
       "      <th>Layer</th>\n",
       "      <th>...</th>\n",
       "      <th>Method</th>\n",
       "      <th>NODCCode</th>\n",
       "      <th>SPECCode</th>\n",
       "      <th>SerialNumber</th>\n",
       "      <th>ProjectIdentifier</th>\n",
       "      <th>DataType</th>\n",
       "      <th>SampleVolume</th>\n",
       "      <th>Units</th>\n",
       "      <th>Salzone</th>\n",
       "      <th>PDepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOBPH</td>\n",
       "      <td>Mobjack Bay-Polyhaline Region</td>\n",
       "      <td>WE4.2</td>\n",
       "      <td>37.24181</td>\n",
       "      <td>-76.38634</td>\n",
       "      <td>C</td>\n",
       "      <td>170820</td>\n",
       "      <td>1/12/2004</td>\n",
       "      <td>10:28:00</td>\n",
       "      <td>AP</td>\n",
       "      <td>...</td>\n",
       "      <td>PH102</td>\n",
       "      <td>07020301</td>\n",
       "      <td>58.0</td>\n",
       "      <td>20041122WE4.</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOBPH</td>\n",
       "      <td>Mobjack Bay-Polyhaline Region</td>\n",
       "      <td>WE4.2</td>\n",
       "      <td>37.24181</td>\n",
       "      <td>-76.38634</td>\n",
       "      <td>C</td>\n",
       "      <td>170820</td>\n",
       "      <td>1/12/2004</td>\n",
       "      <td>10:28:00</td>\n",
       "      <td>AP</td>\n",
       "      <td>...</td>\n",
       "      <td>PH102</td>\n",
       "      <td>07030501</td>\n",
       "      <td>97.0</td>\n",
       "      <td>20041122WE4.</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB6PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB6.4</td>\n",
       "      <td>37.23653</td>\n",
       "      <td>-76.20799</td>\n",
       "      <td>C</td>\n",
       "      <td>170822</td>\n",
       "      <td>1/12/2004</td>\n",
       "      <td>12:25:00</td>\n",
       "      <td>AP</td>\n",
       "      <td>...</td>\n",
       "      <td>PH102</td>\n",
       "      <td>07030101</td>\n",
       "      <td>77.0</td>\n",
       "      <td>20041122CB6.</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOBPH</td>\n",
       "      <td>Mobjack Bay-Polyhaline Region</td>\n",
       "      <td>WE4.2</td>\n",
       "      <td>37.24181</td>\n",
       "      <td>-76.38634</td>\n",
       "      <td>C</td>\n",
       "      <td>170820</td>\n",
       "      <td>1/12/2004</td>\n",
       "      <td>10:28:00</td>\n",
       "      <td>BP</td>\n",
       "      <td>...</td>\n",
       "      <td>PH102</td>\n",
       "      <td>07020205</td>\n",
       "      <td>156.0</td>\n",
       "      <td>20041122WE4.</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>M</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB6PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB6.4</td>\n",
       "      <td>37.23653</td>\n",
       "      <td>-76.20799</td>\n",
       "      <td>C</td>\n",
       "      <td>170822</td>\n",
       "      <td>1/12/2004</td>\n",
       "      <td>12:25:00</td>\n",
       "      <td>AP</td>\n",
       "      <td>...</td>\n",
       "      <td>PH102</td>\n",
       "      <td>0703100114</td>\n",
       "      <td>105.0</td>\n",
       "      <td>20041122CB6.</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>PHYTP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93462</th>\n",
       "      <td>CB6PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB6.1</td>\n",
       "      <td>37.58847</td>\n",
       "      <td>-76.16216</td>\n",
       "      <td>C</td>\n",
       "      <td>644643</td>\n",
       "      <td>12/8/2021</td>\n",
       "      <td>13:20:00</td>\n",
       "      <td>BP</td>\n",
       "      <td>...</td>\n",
       "      <td>PP101</td>\n",
       "      <td>AUTO_PICO</td>\n",
       "      <td>1148</td>\n",
       "      <td>20211208CB6.1</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>M</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93463</th>\n",
       "      <td>CB7PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB7.3E</td>\n",
       "      <td>37.22875</td>\n",
       "      <td>-76.05383</td>\n",
       "      <td>C</td>\n",
       "      <td>644649</td>\n",
       "      <td>12/9/2021</td>\n",
       "      <td>10:32:00</td>\n",
       "      <td>BP</td>\n",
       "      <td>...</td>\n",
       "      <td>PP101</td>\n",
       "      <td>AUTO_PICO</td>\n",
       "      <td>1148</td>\n",
       "      <td>20211209CB7.3E</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>P</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93464</th>\n",
       "      <td>CB7PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB7.3E</td>\n",
       "      <td>37.22875</td>\n",
       "      <td>-76.05383</td>\n",
       "      <td>C</td>\n",
       "      <td>644649</td>\n",
       "      <td>12/9/2021</td>\n",
       "      <td>10:32:00</td>\n",
       "      <td>AP</td>\n",
       "      <td>...</td>\n",
       "      <td>PP101</td>\n",
       "      <td>AUTO_PICO</td>\n",
       "      <td>1148</td>\n",
       "      <td>20211209CB7.3E</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>P</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93465</th>\n",
       "      <td>CB6PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB6.4</td>\n",
       "      <td>37.23653</td>\n",
       "      <td>-76.20799</td>\n",
       "      <td>C</td>\n",
       "      <td>644647</td>\n",
       "      <td>12/9/2021</td>\n",
       "      <td>13:57:00</td>\n",
       "      <td>BP</td>\n",
       "      <td>...</td>\n",
       "      <td>PP101</td>\n",
       "      <td>AUTO_PICO</td>\n",
       "      <td>1148</td>\n",
       "      <td>20211209CB6.4</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>P</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93466</th>\n",
       "      <td>CB6PH</td>\n",
       "      <td>Chesapeake Bay-Polyhaline Region</td>\n",
       "      <td>CB6.4</td>\n",
       "      <td>37.23653</td>\n",
       "      <td>-76.20799</td>\n",
       "      <td>C</td>\n",
       "      <td>644647</td>\n",
       "      <td>12/9/2021</td>\n",
       "      <td>13:57:00</td>\n",
       "      <td>AP</td>\n",
       "      <td>...</td>\n",
       "      <td>PP101</td>\n",
       "      <td>AUTO_PICO</td>\n",
       "      <td>1148</td>\n",
       "      <td>20211209CB6.4</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>PICOP</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Liter</td>\n",
       "      <td>P</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93467 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CBSeg2003              CBSeg2003Description Station  Latitude  \\\n",
       "0        MOBPH      Mobjack Bay-Polyhaline Region   WE4.2  37.24181   \n",
       "1        MOBPH      Mobjack Bay-Polyhaline Region   WE4.2  37.24181   \n",
       "2        CB6PH   Chesapeake Bay-Polyhaline Region   CB6.4  37.23653   \n",
       "3        MOBPH      Mobjack Bay-Polyhaline Region   WE4.2  37.24181   \n",
       "4        CB6PH   Chesapeake Bay-Polyhaline Region   CB6.4  37.23653   \n",
       "...         ...                               ...     ...       ...   \n",
       "93462    CB6PH   Chesapeake Bay-Polyhaline Region   CB6.1  37.58847   \n",
       "93463    CB7PH   Chesapeake Bay-Polyhaline Region  CB7.3E  37.22875   \n",
       "93464    CB7PH   Chesapeake Bay-Polyhaline Region  CB7.3E  37.22875   \n",
       "93465    CB6PH   Chesapeake Bay-Polyhaline Region   CB6.4  37.23653   \n",
       "93466    CB6PH   Chesapeake Bay-Polyhaline Region   CB6.4  37.23653   \n",
       "\n",
       "       Longitude SampleType FieldActivityId SampleDate SampleTime Layer  ...  \\\n",
       "0      -76.38634          C          170820  1/12/2004   10:28:00    AP  ...   \n",
       "1      -76.38634          C          170820  1/12/2004   10:28:00    AP  ...   \n",
       "2      -76.20799          C          170822  1/12/2004   12:25:00    AP  ...   \n",
       "3      -76.38634          C          170820  1/12/2004   10:28:00    BP  ...   \n",
       "4      -76.20799          C          170822  1/12/2004   12:25:00    AP  ...   \n",
       "...          ...        ...             ...        ...        ...   ...  ...   \n",
       "93462  -76.16216          C          644643  12/8/2021   13:20:00    BP  ...   \n",
       "93463  -76.05383          C          644649  12/9/2021   10:32:00    BP  ...   \n",
       "93464  -76.05383          C          644649  12/9/2021   10:32:00    AP  ...   \n",
       "93465  -76.20799          C          644647  12/9/2021   13:57:00    BP  ...   \n",
       "93466  -76.20799          C          644647  12/9/2021   13:57:00    AP  ...   \n",
       "\n",
       "       Method    NODCCode  SPECCode    SerialNumber ProjectIdentifier  \\\n",
       "0       PH102    07020301      58.0    20041122WE4.             PHYTP   \n",
       "1       PH102    07030501      97.0    20041122WE4.             PHYTP   \n",
       "2       PH102    07030101      77.0    20041122CB6.             PHYTP   \n",
       "3       PH102    07020205     156.0    20041122WE4.             PHYTP   \n",
       "4       PH102  0703100114     105.0    20041122CB6.             PHYTP   \n",
       "...       ...         ...       ...             ...               ...   \n",
       "93462   PP101   AUTO_PICO      1148   20211208CB6.1             PICOP   \n",
       "93463   PP101   AUTO_PICO      1148  20211209CB7.3E             PICOP   \n",
       "93464   PP101   AUTO_PICO      1148  20211209CB7.3E             PICOP   \n",
       "93465   PP101   AUTO_PICO      1148   20211209CB6.4             PICOP   \n",
       "93466   PP101   AUTO_PICO      1148   20211209CB6.4             PICOP   \n",
       "\n",
       "      DataType SampleVolume  Units Salzone PDepth  \n",
       "0        PHYTP         15.0  Liter       M    3.0  \n",
       "1        PHYTP         15.0  Liter       M    3.0  \n",
       "2        PHYTP         15.0  Liter       M    3.0  \n",
       "3        PHYTP         15.0  Liter       M   12.5  \n",
       "4        PHYTP         15.0  Liter       M    3.0  \n",
       "...        ...          ...    ...     ...    ...  \n",
       "93462    PICOP         15.0  Liter       M   12.5  \n",
       "93463    PICOP         15.0  Liter       P   18.5  \n",
       "93464    PICOP         15.0  Liter       P    1.0  \n",
       "93465    PICOP         15.0  Liter       P    9.5  \n",
       "93466    PICOP         15.0  Liter       P    3.0  \n",
       "\n",
       "[93467 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file =\"../data/plank_ChesapeakeTidalPlankton.csv\"\n",
    "merge_and_save_data(plankton_monitor_events_dict, plankton_records_dict, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
