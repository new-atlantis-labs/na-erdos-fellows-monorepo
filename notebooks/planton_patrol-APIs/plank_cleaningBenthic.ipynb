{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a database of the water in the Chesapeake Bay. In the [Preparing to combine datasets section](#preparing-to-combine-datasets), run either [turn parameters into columns](#turn-parameters-into-columns) or [renaming columns instead of paramenters into columns](#renaming-columns-instead-of-paramenters-into-columns), as these are two different ways of preparting for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The CSVs have the following columns. The individual CSVs will not have every column:\n",
    "\n",
    "\n",
    "Here is a descriptor of the columns, from [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf):\n",
    "- `CBSeg2003` 2003 Chesapeake Bay Segment Designation. Divided into regions based on circulation and salinity properties. We used 8 from the Bay proper, 2 adjoining Bays, and 1 adjoining sound.\n",
    "- `CBSeg2003Description` 2003 Chesapeake Bay Segment Designation Description in the format Location-Salinity. The locations are Chesapeake Bay, Eastern Bay, Mobjack Bay, and Tangier Sound. The salinity levels are tidal fresh (0.0 - 0.5 parts per thousand),\n",
    "oligohaline (0.5 - 5.0 parts per thousand), mesohaline (5.0 - 18.0 parts per thousand), and polyhaline (greater than 18.0 parts per thousand). \n",
    "- `Station` the sampling station\n",
    "- `Latitude` and  `Longitude`, the Latitude and Longitude for the sampling station\n",
    "- `FieldActivityId` and `EventId` are Database Generated Event Identification Numbers. Theses values are distint-- `FieldActivityId` is adding from the monitoring event data. Sediment and have `EventId` as well.\n",
    "- `SampleDate` Sampling date (MM/DD/YYYY). \n",
    "- `SampleTime` Sample Collection Time (HHMM)\n",
    "- `Layer` Layer of Water Column in Which Sample Was Taken. However, this column is not consistently coded.\n",
    "- `TotalDepth` is Total Station Depth (Meters)\n",
    "- `Source` Data Collection Agency\n",
    "- `SampleReplicate` This parameter combines the sample replicate number with a sample type\n",
    "descriptor. \n",
    "     - S1, Sample 1. The vast majority of the data.\n",
    "     - S2, Sample 2 \n",
    "- `ReportingParameter` (sediment), `IBIParameter` (biomass) Sampling Parameter. In this dataset, all are COUNT,  the number of cells per liter\n",
    "- `ReportingValue` (taxonomic), `ReportedValue` (sediment), or `IBIValue` (bio mass) the value of the parameter.\n",
    "- `ReportingUnits` This parameter describes the units in which a substance is measured. \n",
    "- `ProjectIdentifier`\n",
    "- `Units` units for the sample volume. Always centimeters (cubic centimeters?)\n",
    "- `SampleVolume` Total Volume of Sample\n",
    "- `PDepth`, Composite Sample Cut Off Depth (meters)\n",
    "- `Salzone`, Salinity Zone\n",
    "     - HM, High Mesohaline =>12 TO 18 parts per thousand\n",
    "     - LM, Low Mesohaline =>5.0 TO 12 parts per thousand\n",
    "     - M, Mesohaline =>5.0 TO 18 parts per thousand\n",
    "     - O, Oligohaline =>0.5 TO 5.0 parts per thousand\n",
    "     - P, Polyhaline =>18 parts per thousand\n",
    "     - TF, Tidal Fresh < 0.5 parts per thousand\n",
    "\n",
    "Additionally, the taxonomic dataset has \n",
    "- `GMethod` Chesapeake Bay Program Gear Method Code. Codes represent information relating to the type of field gear used to collect samples for all analysis. In this dataset all are 7, Plankton Pump\n",
    "- `TSN` ITIS Taxon Serial Number, unique to the species. When used in conjunction with the NODC, the TSN\n",
    "overcomes the problem of numeric changes in the NODC code whenever species are reclassified. \n",
    "- `LatinName` Species Latin Name \n",
    "- `Size` Cell Size Groupings when taken. Some species have different measurements for different sizes. \n",
    "- `LifeStageDescription`, a numeric code of the life stage. Most are 89 - not specified.\n",
    "\n",
    "For initial cleaning, we will remove `Layer`, `Units`. Other cleaning will be done based on the dataset.\n",
    "\n",
    "Then we will turn the paramenters into columns populated by the measured values.\n",
    "\n",
    "Finally, combine all four datasets on the values for them monitoring data (other than layer and unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Individual Datasets\n",
    "\n",
    "The first step of data cleaning is working with the individual datasets and determining what information we need to keep. Some of this cleaning will be done with the aid of the DataWrangler extension in VSCode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sediment\n",
    "\n",
    "Opening `../data/plank_ChesapeakeBenthicSediment.csv` in DataWrangler shows that `SampleVolume` is missing 56 values. Let's replace '' and NaN with None and check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment = pd.read_csv('../data/plank_ChesapeakeBenthicSediment.csv')\n",
    "\n",
    "sediment_clean = sediment.replace('', np.nan).where(sediment.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4725, 19)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sediment_clean = sediment_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "sediment_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ReportingParameter` and `ReportingUnits` columns to create a dictionary, then remove the `ReportingUnits` column. We need to find the unique values in `ReportingParameter` to create a dictionary of their meanings. The information is in a PDF which cannot be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MOIST' 'SAND' 'TC' 'TIC' 'SILTCLAY' 'TOC' 'TN' 'VOLORG' 'KURTOSIS'\n",
      " 'CLAY' 'MEANDIAM' 'SORT' 'SKEWNESS']\n"
     ]
    }
   ],
   "source": [
    "sediment_parameters = sediment_clean['ReportingParameter'].unique()\n",
    "\n",
    "print(sediment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_parameters_meanings = ['Sediment Moisture Percentage', 'Sand Content, Percent', 'Total Carbon Content','Total Inorganic Carbonate Content','Silt Clay Content, Percent','Total Organic Carbon','Total Nitrogen','Volatile Organic, Percent','Kurtosis','Clay Content, Percent','Mean Sediment Diameter','Sorting','Skewness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from parameters to their meanings\n",
    "param_to_meaning = {param: meaning for param, meaning in zip(sediment_parameters, sediment_parameters_meanings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial dictionary with units and empty types\n",
    "sediment_param_dict = {param: {'Units': unit, 'Type': \"\"} for param, unit in zip(sediment_clean['ReportingParameter'], sediment_clean['ReportingUnits'])}\n",
    "\n",
    "# Update the dictionary with the meanings\n",
    "for param in sediment_param_dict:\n",
    "    if param in param_to_meaning:\n",
    "        sediment_param_dict[param]['Type'] = param_to_meaning[param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_clean = sediment_clean.drop(columns='ReportingUnits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename `ReportingParameter` as `ReportedParameter`. This will allow us to define a tranformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_clean = sediment_clean.rename(columns={'ReportingParameter':'ReportedParameter'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass = pd.read_csv('../data/plank_ChesapeakeBenthicBioMass.csv')\n",
    "\n",
    "biomass_clean = biomass.replace('', np.nan).where(biomass.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26904, 18)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biomass_clean = biomass_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "biomass_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `IBIParameter` and `IBIValue` columns to create a dictionary, then remove the `IBIValue` column. We need to find the unique values in `IBIParameter` to create a dictionary of their meanings. The information is in a PDF which cannot be scraped, but ther are 126 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_parameters = biomass_clean['IBIParameter'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a few rows that do not encode information that were missed by the cleaning in the download. Let's drop the rows where `IBIParameter` is missing (from DataWrangler, these are the correct rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_clean = biomass_clean.dropna(subset=['IBIParameter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename `IBIParameter` as `ReportedParameter` and `IBIValue` as `ReportedValue`. This will allow us to define a tranformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_clean = biomass_clean.rename(columns={'IBIParameter':'ReportedParameter','IBIValue':'ReportedValue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic Counts\n",
    "\n",
    "Opening `../data/plank_ChesapeakeBenthicTaxonomic.csv` in DataWrangler shows there are also a few rows that do not encode information that were missed by the cleaning in the download. Let's drop the rows where `LatinName` is missing (from DataWrangler, these are the correct rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic = pd.read_csv('../data/plank_ChesapeakeBenthicTaxonomic.csv')\n",
    "\n",
    "taxonomic_clean = taxonomic.replace('', np.nan).where(taxonomic.notna(), None)\n",
    "\n",
    "taxonomic_clean = taxonomic_clean.dropna(subset=['LatinName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26128, 22)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomic_clean = taxonomic_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "taxonomic_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary for `LifeStageDescription`, from the table in the user guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.,  79., 248.,  76.,  97., 247., 245.,  53.,  21., 225.,  93.,\n",
       "        52., 232.])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomic_clean['LifeStageDescription'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_stage_dict = {\n",
    "    89.: 'Not Specified',  \n",
    "    79.: 'Species', \n",
    "    248.: 'Immature Without Cap. Chaete',  \n",
    "    76.: 'Group',  \n",
    "    97.: 'Larvae', \n",
    "    247.: 'Immature With Cap. Chaete', \n",
    "    245.: 'Type',  \n",
    "    53.: 'Species B',  \n",
    "    21.: 'Pupae', \n",
    "    225.: 'Complex',  \n",
    "    93.: 'Juvenile',\n",
    "    52.: 'Species A', \n",
    "    232.: 'Species M'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `LatinName` and `TSN` are already categorical data, we will still with the words version. Let's replace the `LifeStateDescription` with the actual description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean['LifeStageDescription'] = taxonomic_clean['LifeStageDescription'].replace(life_stage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many `LatinNames` are measured at multiple life stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LatinName</th>\n",
       "      <th>LifeStageDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>Ampelisca</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>Ampelisca</td>\n",
       "      <td>Juvenile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Axarus</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>Axarus</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>Bivalvia</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11076</th>\n",
       "      <td>Bivalvia</td>\n",
       "      <td>Species B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Larvae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Pupae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Chironomidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25684</th>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11332</th>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>Gastropoda</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>Gastropoda</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25546</th>\n",
       "      <td>Maldanidae</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>Maldanidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>Nemertea</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Nemertea</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15988</th>\n",
       "      <td>Nereidae</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12892</th>\n",
       "      <td>Nereidae</td>\n",
       "      <td>Species A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13813</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15754</th>\n",
       "      <td>Oligochaeta</td>\n",
       "      <td>Species M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Trepaxonemata</td>\n",
       "      <td>Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11664</th>\n",
       "      <td>Trepaxonemata</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tubificidae</td>\n",
       "      <td>Immature Without Cap. Chaete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Tubificidae</td>\n",
       "      <td>Immature With Cap. Chaete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Tubificoides</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>Tubificoides</td>\n",
       "      <td>Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LatinName          LifeStageDescription\n",
       "24636      Ampelisca                       Species\n",
       "8691       Ampelisca                      Juvenile\n",
       "46            Axarus                 Not Specified\n",
       "641           Axarus                       Species\n",
       "11432       Bivalvia                       Species\n",
       "11076       Bivalvia                     Species B\n",
       "33      Chironomidae                        Larvae\n",
       "351     Chironomidae                         Pupae\n",
       "629     Chironomidae                 Not Specified\n",
       "25684  Enchytraeidae                 Not Specified\n",
       "11332  Enchytraeidae                       Species\n",
       "9884      Gastropoda                       Species\n",
       "3403      Gastropoda                 Not Specified\n",
       "25546     Maldanidae                 Not Specified\n",
       "9470      Maldanidae                       Species\n",
       "8723        Nemertea                       Species\n",
       "41          Nemertea                 Not Specified\n",
       "15988       Nereidae                       Species\n",
       "12892       Nereidae                     Species A\n",
       "22       Oligochaeta                 Not Specified\n",
       "13813    Oligochaeta                       Species\n",
       "15754    Oligochaeta                     Species M\n",
       "192    Trepaxonemata                 Not Specified\n",
       "11664  Trepaxonemata                       Species\n",
       "3        Tubificidae  Immature Without Cap. Chaete\n",
       "168      Tubificidae     Immature With Cap. Chaete\n",
       "312     Tubificoides                       Species\n",
       "8701    Tubificoides                         Group"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by LatinName and check the unique values in LifeStageDescriptions\n",
    "groups = taxonomic_clean.groupby('LatinName')['LifeStageDescription'].nunique()\n",
    "\n",
    "# Identify LatinName values where LifeStageDescriptions has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['LatinName'].isin(disagreeing_a_values)]\n",
    "\n",
    "filtered_df[['LatinName','LifeStageDescription']].drop_duplicates().sort_values(by='LatinName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the `LatinName` and `LifeStageDescription` columns. We will drop `Not Specified`. It appears that in this dataset, any `LatinName` has at most one `LifeStageDescription` that contains `Species`, but we will keep those values to be safe, since some have 'Not Specified'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean['LatinName'] = taxonomic_clean['LatinName'] + ' ' +taxonomic_clean['LifeStageDescription'].replace('Not Specified', '')\n",
    "\n",
    "taxonomic_clean= taxonomic_clean.drop(columns='LifeStageDescription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are `FieldActivityID` and `EventId` different numbering systems for the same thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FieldActivityId</th>\n",
       "      <th>EventId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FieldActivityId, EventId]\n",
       "Index: []"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by FieldActivityId and check the unique values in EventId\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['EventId'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['FieldActivityId'].isin(disagreeing_a_values)]\n",
    "\n",
    "filtered_df[['FieldActivityId','EventId']].drop_duplicates().sort_values(by='FieldActivityId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are! Do they include depth? time? GMethod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalDepth (0, 2)\n",
      "PDepth (0, 2)\n",
      "SampleTime (0, 2)\n",
      "GMethod (0, 2)\n"
     ]
    }
   ],
   "source": [
    "# Group by FieldActivityId and check the unique values in TotalDepth\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['TotalDepth'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where TotalDepth has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['FieldActivityId'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('TotalDepth',filtered_df[['FieldActivityId','TotalDepth']].drop_duplicates().sort_values(by='FieldActivityId').shape)\n",
    "\n",
    "# Group by FieldActivityId and check the unique values in PDepth\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['PDepth'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where TotalDepth has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['FieldActivityId'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('PDepth',filtered_df[['FieldActivityId','PDepth']].drop_duplicates().sort_values(by='FieldActivityId').shape)\n",
    "\n",
    "# Group by FieldActivityId and check the unique values in SampleTime\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['SampleTime'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['SampleTime'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('SampleTime',filtered_df[['FieldActivityId','SampleTime']].drop_duplicates().sort_values(by='SampleTime').shape)\n",
    "\n",
    "# Group by FieldActivityId and check the unique values in GMethod\n",
    "groups = taxonomic_clean.groupby('FieldActivityId')['GMethod'].nunique()\n",
    "\n",
    "# Identify FieldActivityId values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = taxonomic_clean[taxonomic_clean['GMethod'].isin(disagreeing_a_values)]\n",
    "\n",
    "print('GMethod',filtered_df[['FieldActivityId','GMethod']].drop_duplicates().sort_values(by='GMethod').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `TSN` always the same for `LatinName`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.drop_duplicates of Empty DataFrame\n",
       "Columns: [CBSeg2003, CBSeg2003Description, Station, Latitude, Longitude, SampleType, FieldActivityId, SampleDate, SampleTime, TotalDepth, ReportingValue, ReportingUnit, EventId, Source, GMethod, TSN, LatinName, ProjectIdentifier, Salzone, PDepth, SampleVolume]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]>"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by LatinName and check the unique values in GMethod\n",
    "groups = taxonomic_clean.groupby('LatinName')['TSN'].nunique()\n",
    "\n",
    "# Identify LatinName values where EventId has more than one unique value\n",
    "disagreeing_a_values = groups[groups > 1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "taxonomic_clean[taxonomic_clean['TSN'].isin(disagreeing_a_values)].drop_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pivot the `LatinName` column and `ReportingValue` column. \n",
    "We drop `TSN` since we are using `LatinName` (the other option would be to also pivot on `TSN` and `ReportingValue`). Since `GMethod` is information about how the sample was collected, and no needed for merging datasets, we will also remove it. \n",
    "\n",
    "The `ReportingUnit` is always `CM`, so we can also drop it. Also, some of these measurements must be counts, unless some clams are over a kilometer in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean = taxonomic_clean.drop(columns=['TSN','GMethod','ReportingUnit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with ReportingValues column -- not used later\n",
    "\n",
    "Since some values in the `ReportingValue` column appear to be counts and some are size, let's see if there are any rows that agree in all columns except `ReportingValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude from comparison\n",
    "exclude_columns = ['ReportingValue']\n",
    "\n",
    "# Columns to compare\n",
    "compare_columns = [col for col in taxonomic_clean.columns if col not in exclude_columns]\n",
    "\n",
    "# Group by the columns to compare\n",
    "grouped = taxonomic_clean.groupby(compare_columns)\n",
    "\n",
    "# Initialize lists to collect rows based on group size\n",
    "groups_with_2_rows = []\n",
    "groups_with_3_rows = []\n",
    "groups_with_more_than_3_rows = []\n",
    "\n",
    "# Iterate through each group\n",
    "for group_key, group in grouped:\n",
    "    num_rows = len(group)\n",
    "    \n",
    "    # Separate groups based on number of rows\n",
    "    if num_rows == 2:\n",
    "        groups_with_2_rows.append(group)\n",
    "    elif num_rows == 3:\n",
    "        groups_with_3_rows.append(group)\n",
    "    elif num_rows > 3:\n",
    "        groups_with_more_than_3_rows.append(group)\n",
    "\n",
    "# Create DataFrames for viewing\n",
    "df_groups_with_2_rows = pd.concat(groups_with_2_rows).sort_values(by=['LatinName', 'EventId']) if groups_with_2_rows else pd.DataFrame()\n",
    "\n",
    "df_groups_with_3_rows = pd.concat(groups_with_3_rows).sort_values(by=['LatinName', 'EventId','ReportingValue']) if groups_with_3_rows else pd.DataFrame()\n",
    "\n",
    "df_groups_with_more_than_3_rows = pd.concat(groups_with_more_than_3_rows).sort_values(by=['LatinName', 'EventId','ReportingValue']).drop_duplicates() if groups_with_more_than_3_rows else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure what to do about the groups with more than two rows, so let's continue with the groups with two rows and come back. For the groups with two rows, we will check if one of the values is an integer. If it is, then that will be the count and the other value will be the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to accumulate results\n",
    "results_list = []\n",
    "error_groups_list = []\n",
    "\n",
    "# Iterate over each group in the existing 'grouped' object\n",
    "for group_key, group in grouped:\n",
    "    if len(group) == 2:\n",
    "        # Extract values from the group\n",
    "        values = group['ReportingValue'].values\n",
    "        \n",
    "        # Check which values are integers\n",
    "        int_values = [v for v in values if v == round(v)]\n",
    "        \n",
    "        if len(int_values) == 1:\n",
    "            # One integer value found\n",
    "            reporting_count = int_values[0]\n",
    "            reporting_size = values[0] if values[0] != reporting_count else values[1]\n",
    "            \n",
    "            # Initialize new columns\n",
    "            group = group.copy()\n",
    "            group['ReportingCount'] = reporting_count\n",
    "            group['ReportingSize'] = reporting_size\n",
    "            \n",
    "            # Append to results list\n",
    "            results_list.append(group)\n",
    "        else:\n",
    "            # Append to error_groups list\n",
    "            error_groups_list.append(group)\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "taxonomic_two_measures = pd.concat(results_list, ignore_index=True) if results_list else pd.DataFrame(columns=taxonomic_clean.columns.tolist() + ['ReportingCount', 'ReportingSize'])\n",
    "\n",
    "error_groups_df = pd.concat(error_groups_list, ignore_index=True) if error_groups_list else pd.DataFrame(columns=taxonomic_clean.columns)\n",
    "\n",
    "# Sort the results DataFrame by 'LatinName' and 'EventId'\n",
    "taxonomic_two_measures.sort_values(by=['LatinName', 'EventId'], inplace=True)\n",
    "\n",
    "# Sort the error groups DataFrame by 'LatinName' and 'EventId'\n",
    "error_groups_df.sort_values(by=['LatinName', 'EventId'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error group contains four rows where every value is an integer. We will handle that case and the groups of size 1 together. Let's generate the groups of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to accumulate single element groups\n",
    "single_element_groups_list = []\n",
    "\n",
    "# Iterate over each group in the existing 'grouped' object\n",
    "for group_key, group in grouped:\n",
    "    if len(group) == 1:\n",
    "        # Append single element groups to single_element_groups_list\n",
    "        single_element_groups_list.append(group)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "taxonomic_one_measure = pd.concat(single_element_groups_list, ignore_index=True) if single_element_groups_list else pd.DataFrame(columns=taxonomic_clean.columns)\n",
    "\n",
    "# Optionally, sort the DataFrame by 'LatinName'\n",
    "taxonomic_one_measure.sort_values(by=['LatinName'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since we will want to update this later, let's make a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranges_dict(dataframe, column_to_fix, ranges_columns):\n",
    "    dictionary = {}\n",
    "    for latin_name, group in dataframe.groupby(column_to_fix):\n",
    "        col_dict = {}\n",
    "        for col in ranges_columns:\n",
    "            if col in group.columns:\n",
    "                # Get the min and max values for each column\n",
    "                reporting_range = [group[col].min(), group[col].max()]\n",
    "                # Add the range to the column dictionary\n",
    "                col_dict[col + 'Range'] = reporting_range\n",
    "        # Store the column dictionary in the main dictionary\n",
    "        dictionary[latin_name] = col_dict\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges_dict = create_ranges_dict(taxonomic_two_measures,'LatinName',['ReportingSize','ReportingCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_in_ranges(row):\n",
    "    latin_name = row['LatinName']\n",
    "    reporting_value = row['ReportingValue']\n",
    "    \n",
    "    if latin_name in ranges_dict:\n",
    "        count_range = ranges_dict[latin_name]['ReportingCountRange']\n",
    "        size_range = ranges_dict[latin_name]['ReportingSizeRange']\n",
    "        \n",
    "        # Check if the reporting_value is within either range\n",
    "        in_count_range = count_range[0] <= reporting_value <= count_range[1]\n",
    "        in_size_range = size_range[0] <= reporting_value <= size_range[1]\n",
    "        \n",
    "        if in_count_range and in_size_range:\n",
    "            return 'Both'\n",
    "        elif in_count_range:\n",
    "            # check if integer\n",
    "            if reporting_value == round(reporting_value):\n",
    "                return 'In Count Range'\n",
    "            else:\n",
    "                return 'In Count Range, not integer'\n",
    "    \n",
    "        elif in_size_range:\n",
    "            return 'In Size Range'\n",
    "        else:\n",
    "            # check if integer\n",
    "            if reporting_value == round(reporting_value):\n",
    "                return 'Not in ranges, integer'\n",
    "            else:\n",
    "                return 'Not in ranges, not integer'\n",
    "    else:\n",
    "        # check if integer\n",
    "        if reporting_value == round(reporting_value):\n",
    "            return 'Taxa not in dictionary, integer'\n",
    "        else:\n",
    "            return 'Taxa not in dictionary, not integer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if the `ReportedValue` is in the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_one_measure['InRange'] = taxonomic_one_measure.apply(check_value_in_ranges, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add some of these values to the correct column to start creating our new dataset. We will remove those rows from `taxonomic_one_measure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows that can be updayed\n",
    "condition = taxonomic_one_measure['InRange'].isin(['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer']) | (taxonomic_one_measure['InRange'] == 'In Count Range')\n",
    "\n",
    "# Create the new DataFrame with all columns and additional columns 'ReportingSize' and 'ReportingCount'\n",
    "new_taxonomic_one_measure = taxonomic_one_measure[condition].copy()\n",
    "new_taxonomic_one_measure['ReportingSize'] = new_taxonomic_one_measure.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] in ['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer'] else None, \n",
    "    axis=1\n",
    ")\n",
    "new_taxonomic_one_measure['ReportingCount'] = new_taxonomic_one_measure.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] == 'In Count Range' else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove the identified rows from the original DataFrame while retaining all columns\n",
    "# Drop InRanges to rerun\n",
    "taxonomic_one_measure = taxonomic_one_measure.loc[~condition].drop(columns='InRange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same thing with the error group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add InRanges\n",
    "error_groups_df['InRange'] = error_groups_df.apply(check_value_in_ranges, axis=1)\n",
    "\n",
    "# Move the values we can\n",
    "# Identify rows that can be updayed\n",
    "condition = error_groups_df['InRange'].isin(['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer']) | (error_groups_df['InRange'] == 'In Count Range')\n",
    "\n",
    "# Create the new DataFrame with all columns and additional columns 'ReportingSize' and 'ReportingCount'\n",
    "new_error_groups_df = error_groups_df[condition].copy()\n",
    "new_error_groups_df['ReportingSize'] = new_error_groups_df.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] in ['In Size Range', 'Not in ranges, not integer', 'Taxa not in dictionary, not integer'] else None, \n",
    "    axis=1\n",
    ")\n",
    "new_error_groups_df['ReportingCount'] = new_error_groups_df.apply(\n",
    "    lambda row: row['ReportingValue'] if row['InRange'] == 'In Count Range' else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove the identified rows from the original DataFrame while retaining all columns\n",
    "# Drop InRanges to rerun\n",
    "error_groups_df = error_groups_df.loc[~condition].drop(columns='InRange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine our dataframes, update the dictionary, and run ranges assessment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rl/kqt6tbv90l9_pwc4927vdb340000gn/T/ipykernel_30729/2626544837.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_taxonomic_data = pd.concat([new_error_groups_df,new_taxonomic_one_measure,taxonomic_two_measures],ignore_index=True).drop(columns=['ReportingValue','InRange']).drop_duplicates()\n"
     ]
    }
   ],
   "source": [
    "combined_taxonomic_data = pd.concat([new_error_groups_df,new_taxonomic_one_measure,taxonomic_two_measures],ignore_index=True).drop(columns=['ReportingValue','InRange']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate our dictionary and try again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Quality\n",
    "\n",
    "Opening `../data/plank_ChesapeakeBenthicWaterQuality.csv` in DataWrangler shows that `SampleVolume` is missing 94 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = pd.read_csv('../data/plank_ChesapeakeBenthicWaterQuality.csv')\n",
    "\n",
    "water_clean = water.replace('', np.nan).where(water.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop `Layer` and `Units` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8427, 21)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_clean = water_clean.drop(columns=['Layer','Units'])\n",
    "\n",
    "water_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ReportedParameter` and `ReportingUnits` columns to create a dictionary, then remove the `ReportingUnits` column. We need to find the unique values in `ReportingParameter` to create a dictionary of their meanings. The information is in a PDF which cannot be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PH' 'WTEMP' 'DO' 'DO_SAT_P' 'SALINITY' 'SPCOND']\n"
     ]
    }
   ],
   "source": [
    "water_parameters = water_clean['ReportedParameter'].unique()\n",
    "\n",
    "print(water_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for the parameters. All are method F01, in-situ measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial dictionary with units and empty types\n",
    "water_param_dict = {'PH':'pH', 'WTEMP' : 'Water Temperature', 'DO': 'Dissolved Oxygen', 'DO_SAT_P': 'Dissolved oxygen relative to theoretical value at saturation (%)', 'SALINITY': 'Salinity in-situ measured with probe', 'SPCOND' :'Specific Conductance At 25 C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_clean = water_clean.drop(columns=['ReportedUnits','WQMethod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining datasets\n",
    "\n",
    "There are two options here: \n",
    "- We can turn the parameters into columns. This way, each parameter has a column with the measured value. We can then combine rows based on the `FieldActivityId`, so that all measurements taking at the same time and location (including depth) are in the same row.\n",
    "- The other option is to rename the parameter and measured value columns for each of the datasets for a consistent naming scheme. Since the different datasets have different columns, we will also need to drop any excess columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for analyzing column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what columns are common to all for cleaned dataframes and which are unique. Since we will want to run this step a few times, we will define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary that stores the variable names\n",
    "def dictionary_variable_names(*dfs):\n",
    "    # create empty dictionary\n",
    "    variable_names = {}\n",
    "    \n",
    "    # iterate over the dataframes\n",
    "    for df in dfs:\n",
    "        # get the variable name\n",
    "        var_name = [var for var, val in globals().items() if val is df][0]\n",
    "        \n",
    "        # add the variable name to the dictionary\n",
    "        variable_names[var_name] = df\n",
    "    \n",
    "    return variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_columns(*dfs):\n",
    "    df_dict = dictionary_variable_names(*dfs)\n",
    "    # Extract column sets\n",
    "    columns_sets = {name: set(df.columns) for name, df in df_dict.items()}\n",
    "    \n",
    "    # Find common columns\n",
    "    common_columns = set.intersection(*columns_sets.values())\n",
    "    \n",
    "    # Find unique columns for each DataFrame\n",
    "    unique_columns = {\n",
    "        name: columns - set.union(*(other_columns for other_name, other_columns in columns_sets.items() if other_name != name))\n",
    "        for name, columns in columns_sets.items()\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Common Columns:\")\n",
    "    print(common_columns)\n",
    "    \n",
    "    for name, df_columns in columns_sets.items():\n",
    "        print(f\"\\nUnique Columns in {name}:\")\n",
    "        print(unique_columns[name])\n",
    "        print(f\"Columns in {name} not in common_columns:\")\n",
    "        print(df_columns - common_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn parameters into columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_to_columns(dataframe,columns_to_group):\n",
    "    # Reset index to use row numbers as the index\n",
    "    df_reset = dataframe.reset_index(drop=True)\n",
    "\n",
    "    # Pivot the DataFrame while preserving non-pivoted columns\n",
    "    df_pivoted = df_reset.pivot_table(index=df_reset.index, columns='ReportedParameter', values='ReportedValue', aggfunc='first')\n",
    "\n",
    "    # Combine pivoted result with the original DataFrame columns not involved in the pivot\n",
    "    df_pivoted = df_reset.drop(columns=['ReportedParameter','ReportedValue']).join(df_pivoted)\n",
    "\n",
    "    #check unique combinations\n",
    "    #this allows us to check that we havent lost data\n",
    "    unique_combinations = df_pivoted[columns_to_group].drop_duplicates()\n",
    "\n",
    "    # Check we haven't lost unique non-empty values\n",
    "    for col in columns_to_group:\n",
    "        # Filter out empty values\n",
    "        df_combined_nonempty = df_pivoted[col].dropna()\n",
    "        df_pivoted_nonempty = dataframe[col].dropna()\n",
    "        \n",
    "        # Check if the number of unique non-empty values matches\n",
    "        unique_check = df_combined_nonempty.unique().size == df_pivoted_nonempty.unique().size\n",
    "        \n",
    "        # Print the results\n",
    "        print(\"Checking unique values in \" ,col, unique_check)\n",
    "    \n",
    "    # Create a copy of the DataFrame for processing\n",
    "    df_processed = df_pivoted.copy()\n",
    "\n",
    "    # Create a unique identifier for each group based on the columns to match\n",
    "    df_processed['UniqueID'] = df_processed[columns_to_group].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "    # Group by the unique identifier\n",
    "    df_combined = df_processed.groupby('UniqueID', as_index=False).first()\n",
    "\n",
    "    # Drop the UniqueID column and remove duplicates\n",
    "    df_really_clean = df_combined.drop(columns='UniqueID').drop_duplicates()\n",
    "\n",
    "    return df_really_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns to group come from the monitoring data, so should be present in all datasets. We will double check this with the `analyze_columns` function from the [functions for analyzing column names](#functions-for-analyzing-column-names) subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Columns:\n",
      "{'PDepth', 'Salzone', 'CBSeg2003Description', 'FieldActivityId', 'CBSeg2003', 'SampleTime', 'Latitude', 'Longitude', 'SampleDate', 'TotalDepth', 'ProjectIdentifier', 'Source', 'Station', 'SampleVolume'}\n",
      "\n",
      "Unique Columns in sediment_clean:\n",
      "set()\n",
      "Columns in sediment_clean not in common_columns:\n",
      "{'ReportedParameter', 'ReportedValue', 'SampleReplicate', 'EventId'}\n",
      "\n",
      "Unique Columns in biomass_clean:\n",
      "{'BiologicalEventId'}\n",
      "Columns in biomass_clean not in common_columns:\n",
      "{'ReportedParameter', 'ReportedValue', 'BiologicalEventId', 'SampleReplicate'}\n",
      "\n",
      "Unique Columns in taxonomic_clean:\n",
      "{'SampleType', 'LatinName', 'ReportingValue'}\n",
      "Columns in taxonomic_clean not in common_columns:\n",
      "{'SampleType', 'ReportingValue', 'LatinName', 'EventId'}\n",
      "\n",
      "Unique Columns in water_clean:\n",
      "{'SampleDepth'}\n",
      "Columns in water_clean not in common_columns:\n",
      "{'ReportedParameter', 'ReportedValue', 'SampleDepth', 'EventId', 'SampleReplicate'}\n"
     ]
    }
   ],
   "source": [
    "analyze_columns(sediment_clean, biomass_clean, taxonomic_clean,water_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_group = ['PDepth', 'Salzone', 'CBSeg2003Description', 'FieldActivityId', 'CBSeg2003', 'SampleTime', 'Latitude', 'Longitude', 'SampleDate', 'TotalDepth', 'ProjectIdentifier', 'Source', 'Station', 'SampleVolume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  SampleTime True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleVolume True\n"
     ]
    }
   ],
   "source": [
    "sediment_really_clean = parameter_to_columns(sediment_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  SampleTime True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleVolume True\n"
     ]
    }
   ],
   "source": [
    "biomass_really_clean = parameter_to_columns(biomass_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some final cleaning for Taxonomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_clean = taxonomic_clean.rename(columns={'ReportingValue': 'ReportedValue','LatinName':'ReportedParameter'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  SampleTime True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleVolume True\n"
     ]
    }
   ],
   "source": [
    "taxonomic_really_clean = parameter_to_columns(taxonomic_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in  PDepth True\n",
      "Checking unique values in  Salzone True\n",
      "Checking unique values in  CBSeg2003Description True\n",
      "Checking unique values in  FieldActivityId True\n",
      "Checking unique values in  CBSeg2003 True\n",
      "Checking unique values in  SampleTime True\n",
      "Checking unique values in  Latitude True\n",
      "Checking unique values in  Longitude True\n",
      "Checking unique values in  SampleDate True\n",
      "Checking unique values in  TotalDepth True\n",
      "Checking unique values in  ProjectIdentifier True\n",
      "Checking unique values in  Source True\n",
      "Checking unique values in  Station True\n",
      "Checking unique values in  SampleVolume True\n"
     ]
    }
   ],
   "source": [
    "water_really_clean = parameter_to_columns(water_clean,columns_to_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of some extra columns before merging. `BiologicalEventId` is only in BioMass, `EventId` is simply a different system for recoding the same information as `FieldActivityId`. `SampleReplicate` should not matter, but migh prevent some merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_really_clean = biomass_really_clean.drop(columns=['SampleReplicate','BiologicalEventId'])\n",
    "taxonomic_really_clean = taxonomic_really_clean.drop(columns=['EventId'])\n",
    "water_really_clean = water_really_clean.drop(columns=['EventId','SampleReplicate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Dataset\n",
    "\n",
    "Now we combine on 'CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
    "       'EventId', 'Source', 'SampleDate', 'SampleDepth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sediment_really_clean and biomass_really_clean\n",
    "merged_df = pd.merge(sediment_really_clean, biomass_really_clean, how='outer', on=[col for col in sediment_really_clean.columns if col in biomass_really_clean.columns], suffixes=('', '_biomass_really_clean'))\n",
    "\n",
    "# Merge the result with taxonomic_really_clean\n",
    "merged_df = pd.merge(merged_df, taxonomic_really_clean, how='outer', on=[col for col in merged_df.columns if col in taxonomic_really_clean.columns and not col.endswith('_biomass_really_clean')], suffixes=('', '_taxonomic_really_clean'))\n",
    "\n",
    "# Merge the result with water_really_clean\n",
    "merged_df = pd.merge(merged_df, water_really_clean, how='outer', on=[col for col in merged_df.columns if col in water_really_clean.columns and not col.endswith(('_biomass_really_clean', '_taxonomic_really_clean'))], suffixes=('', '_water_really_clean'))\n",
    "\n",
    "# Reset the index for better readability\n",
    "merged_df = merged_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some issues with different precisions for latitude and longitude causing lack of matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define columns for matching\n",
    "match_columns = [\n",
    "    'CBSeg2003', 'CBSeg2003Description', 'Station', 'FieldActivityId', 'SampleDate',\n",
    "    'SampleTime', 'TotalDepth', 'Source', 'ProjectIdentifier'\n",
    "]\n",
    "\n",
    "# Generate a composite key based on the matching columns\n",
    "merged_df['unique_key'] = merged_df[match_columns].apply(lambda row: tuple(row.fillna('missing')), axis=1)\n",
    "\n",
    "# Handle Latitude and Longitude with precision\n",
    "# Keep the most precise value for Latitude and Longitude\n",
    "merged_df['Latitude'] = merged_df.groupby('unique_key')['Latitude'].transform(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "merged_df['Longitude'] = merged_df.groupby('unique_key')['Longitude'].transform(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "\n",
    "# Aggregate the groups\n",
    "result_df = merged_df.groupby('unique_key').first().reset_index()\n",
    "\n",
    "# Drop the unique_key column from the result\n",
    "result_df = result_df.drop(columns=['unique_key'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's strip whitespace, which should have been done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in result_df.columns:\n",
    "    if result_df[col].dtype == 'object':\n",
    "        result_df[col] = result_df[col].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('../data/plank_ChesapeakeBayBenthic_clean_wide.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns instead of paramenters into columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure all of the datasets have the same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Columns:\n",
      "{'PDepth', 'ReportedParameter', 'Salzone', 'CBSeg2003Description', 'FieldActivityId', 'CBSeg2003', 'SampleTime', 'ReportedValue', 'Latitude', 'Longitude', 'SampleDate', 'TotalDepth', 'ProjectIdentifier', 'Source', 'Station', 'SampleVolume'}\n",
      "\n",
      "Unique Columns in sediment_clean:\n",
      "set()\n",
      "Columns in sediment_clean not in common_columns:\n",
      "{'SampleReplicate', 'EventId'}\n",
      "\n",
      "Unique Columns in biomass_clean:\n",
      "{'BiologicalEventId'}\n",
      "Columns in biomass_clean not in common_columns:\n",
      "{'BiologicalEventId', 'SampleReplicate'}\n",
      "\n",
      "Unique Columns in taxonomic_clean:\n",
      "{'SampleType'}\n",
      "Columns in taxonomic_clean not in common_columns:\n",
      "{'SampleType', 'EventId'}\n",
      "\n",
      "Unique Columns in water_clean:\n",
      "{'SampleDepth'}\n",
      "Columns in water_clean not in common_columns:\n",
      "{'SampleReplicate', 'SampleDepth', 'EventId'}\n"
     ]
    }
   ],
   "source": [
    "analyze_columns(sediment_clean,biomass_clean,taxonomic_clean,water_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `EventId` encodes the same information as `FieldActivityId`, but no used by every dataset, we will drop it. Similar with `BiologicalEventId`. `SampleReplicate` is another column that would be helpful to double check if we were combining rows, but is not present in every dataset, so we will drop it. Let"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "sediment_really_clean = sediment_clean.drop(columns=['EventId','SampleReplicate'])\n",
    "biomass_really_clean = biomass_clean.drop(columns=['BiologicalEventId','SampleReplicate'])\n",
    "taxonomic_really_clean = taxonomic_clean.drop(columns=['EventId','SampleType'])\n",
    "water_really_clean = water_clean.drop(columns=['EventId','SampleDepth','SampleReplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Columns:\n",
      "{'PDepth', 'ReportedParameter', 'Salzone', 'CBSeg2003Description', 'FieldActivityId', 'CBSeg2003', 'SampleTime', 'ReportedValue', 'Latitude', 'Longitude', 'SampleDate', 'TotalDepth', 'ProjectIdentifier', 'Source', 'Station', 'SampleVolume'}\n",
      "\n",
      "Unique Columns in sediment_really_clean:\n",
      "set()\n",
      "Columns in sediment_really_clean not in common_columns:\n",
      "set()\n",
      "\n",
      "Unique Columns in biomass_really_clean:\n",
      "set()\n",
      "Columns in biomass_really_clean not in common_columns:\n",
      "set()\n",
      "\n",
      "Unique Columns in taxonomic_really_clean:\n",
      "set()\n",
      "Columns in taxonomic_really_clean not in common_columns:\n",
      "set()\n",
      "\n",
      "Unique Columns in water_really_clean:\n",
      "set()\n",
      "Columns in water_really_clean not in common_columns:\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "analyze_columns(sediment_really_clean,biomass_really_clean,taxonomic_really_clean,water_really_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use a consistent naming convention. Let's use  `ReportedParameter` and `ReportedValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomic_really_clean = taxonomic_clean.rename(columns={'LatinName':'ReportedParameter','ReportingValue':'ReportedValue'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Columns:\n",
      "{'PDepth', 'ReportedParameter', 'Salzone', 'CBSeg2003Description', 'FieldActivityId', 'CBSeg2003', 'SampleTime', 'ReportedValue', 'Latitude', 'Longitude', 'SampleDate', 'TotalDepth', 'ProjectIdentifier', 'Source', 'Station', 'SampleVolume'}\n",
      "\n",
      "Unique Columns in sediment_really_clean:\n",
      "set()\n",
      "Columns in sediment_really_clean not in common_columns:\n",
      "set()\n",
      "\n",
      "Unique Columns in biomass_really_clean:\n",
      "set()\n",
      "Columns in biomass_really_clean not in common_columns:\n",
      "set()\n",
      "\n",
      "Unique Columns in taxonomic_really_clean:\n",
      "{'SampleType', 'EventId'}\n",
      "Columns in taxonomic_really_clean not in common_columns:\n",
      "{'SampleType', 'EventId'}\n",
      "\n",
      "Unique Columns in water_really_clean:\n",
      "set()\n",
      "Columns in water_really_clean not in common_columns:\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "analyze_columns(sediment_really_clean,biomass_really_clean,taxonomic_really_clean,water_really_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can concatinate the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([sediment_really_clean,biomass_really_clean,taxonomic_really_clean,water_really_clean], ignore_index=True)\n",
    "\n",
    "for col in combined_df.columns:\n",
    "    if combined_df[col].dtype == 'object':\n",
    "        combined_df[col] = combined_df[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason 'EventId','SampleType' did not drop, so let's do that again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['EventId','SampleType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('../data/plank_ChesapeakeBayBenthic_clean_tall.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
