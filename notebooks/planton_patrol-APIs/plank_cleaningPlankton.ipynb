{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a database of the plankton in the Chesapeake Bay.\n",
    "\n",
    "The CSV from the API (combined with station data) has the columns \n",
    "`CBSeg2003`, `CBSeg2003Description`, `Station`, `Latitude`, `Longitude`,\n",
    "`FieldActivityId`, `Source`, `SampleType`, `SampleDate`, `Layer`,\n",
    "`SampleNumber`, `GMethod`, `TSN`, `LatinName`, `Size`, `Method`,\n",
    "`Parameter`, `ReportingValue`, `ReportingUnit`, `NODCCode`, `SPECCode`,\n",
    "`SerialNumber`\n",
    "\n",
    "Here is a descriptor of the columns, from [The 2012 Users Guide to CBP Biological Monitoring Data](https://d18lev1ok5leia.cloudfront.net/chesapeakebay/documents/guide2012_final.pdf):\n",
    "- `CBSeg2003` 2003 Chesapeake Bay Segment Designation. Divided into regions based on circulation and salinity properties. We used 8 from the Bay proper, 2 adjoining Bays, and 1 adjoining sound.\n",
    "- `CBSeg2003Description` 2003 Chesapeake Bay Segment Designation Description in the format Location-Salinity. The locations are Chesapeake Bay, Eastern Bay, Mobjack Bay, and Tangier Sound. The salinity levels are tidal fresh (0.0 - 0.5 parts per thousand),\n",
    "oligohaline (0.5 - 5.0 parts per thousand), mesohaline (5.0 - 18.0 parts per thousand), and polyhaline (greater than 18.0 parts per thousand). \n",
    "- `Station` the sampling station\n",
    "- `Latitude` and  `Longitude`, the Latitude and Longitude for the sampling station\n",
    "- 'FieldActivityId' is not included in the database user guide\n",
    "- `Source` Data Collection Agency\n",
    "- `SampleType`  Collection Type. However, in this dataset all are C, composite sample, made up of subsamples from multiple depths.\n",
    "- `SampleDate` Sampling date (MM/DD/YYYY). We downloaded 8/9/2004 through 12/9/2021\n",
    "- `Layer` Layer of Water Column in Which Sample Was Taken. In this dataset\n",
    "     - S, Surface\n",
    "     - AP, Above pycnocline\n",
    "     - WC, Whole water column\n",
    "- `SampleNumber` number assigned to the sample\n",
    "- `GMethod` Chesapeake Bay Program Gear Method Code. Codes represent information relating to the type of field gear used to collect samples for all analysis. In this dataset all are 7, Plankton Pump\n",
    "- `TSN` ITIS Taxon Serial Number, unique to the species. When used in conjunction with the NODC, the TSN\n",
    "overcomes the problem of numeric changes in the NODC code whenever species are reclassified. \n",
    "- `LatinName` Species Latin Name \n",
    "- `Size` Cell Size Groupings when taken. Some species have different measurements for different sizes. \n",
    "- `Method` Chesapeake Bay Program Sample Analysis Code. In January of 2005 in Maryland and October 2005 in Virginia, the following enumeration technique was instituted for all Chesapeake Bay Program supported phytoplankton enumerations. In this sample, the codes are\n",
    "     - PH101, MSU/ANS Phytoplankton Enumeration Method\n",
    "     - PH102, ODU Phytoplankton Enumeration Method\n",
    "     - PH102M, ODU Phytoplankton Enumeration Method-2005 Modification\n",
    "     - PH103, Uniform Chesapeake Bay Program Phytoplankton Enumeration Method\n",
    "     - PP101, ODU Picoplankton Enumeration Method\n",
    "     - PP102, MSU/ANS Picoplankton Enumeration Method \n",
    "- `Parameter` Sampling Parameter. In this dataset, all are COUNT,  the number of cells per liter\n",
    "- `ReportingValue` the value of the count\n",
    "- `ReportingUnit` This parameter describes the units in which a substance is measured. In this dataset, all are L.\n",
    "- `NODCCode` National Oceanographic Data Center Species Code. All species on the list have been assigned at least partial National Oceanographic Data Center (NODC).\n",
    "- `SPECCode` Many of the agencies reporting data containing species information have developed their own in-house species codes. All of these codes are found in the SPECCODE column of a given data type. Codes will\n",
    "vary by agency and data type. The agency code column in most cases has been given the agency name\n",
    "code in the data documentation. \n",
    "- `SerialNumber` Sample serial number. However, multiple dates and locations have the same serial number.\n",
    "\n",
    "Since there are more unique Latin names than TSN (563 vs 519), we will use Latin. There are many missing NODC Codes and SPEC Codes. In theory, these four columns all encode the same data.\n",
    "\n",
    "The main thing we will need to do is create a column for each of the 563 unique Latin names. Then put the count in the correct row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93467, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "plankton = pd.read_csv('../data/plank_ChesapeakeTidalPlankton.csv',low_memory=False)\n",
    "\n",
    "print(plankton.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dataframe size\n",
    "\n",
    "First, let's remove the excess columns:\n",
    "- `Source` is a bookkeepng value. Since we do not need to combine datasets, we can remove this value.\n",
    "- `SampleType`,`GMethod`, `Parameter`, and `ReportingUnit` are the same (or empty) for every entry\n",
    "- `NODCCode` and `SPECCode` encode the same information as `TSN` and `LatinName`, with a lot of missing values. The User Guide says `TSN` is preferable to `NODCCode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton_clean = plankton.drop(columns=['Source','SampleType','GMethod', 'Parameter', 'ReportingUnit','NODCCode', 'SPECCode'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There also might be rows that do not encode information that were missed by the cleaning in the download. Let's drop the rows where `LatinName` is missing (from DataWrangler, these are the correct rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton_clean = plankton_clean.dropna(subset=['LatinName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93466, 23)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plankton_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
       "       'FieldActivityId', 'SampleDate', 'SampleTime', 'Layer', 'TotalDepth',\n",
       "       'ReportingValue', 'SampleNumber', 'TSN', 'LatinName', 'Size', 'Method',\n",
       "       'SerialNumber', 'ProjectIdentifier', 'Units', 'DataType',\n",
       "       'SampleVolume', 'PDepth', 'Salzone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plankton_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features columns\n",
    "\n",
    "First, let's handle the face that some species are measured at various sizes. We will combine the `LatinName` and `Size` columns, removing `Not Applicable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93466, 22)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plankton_clean['LatinName'] = plankton_clean['LatinName'] + ' ' +plankton_clean['Size'].replace('Not Applicable', '')\n",
    "\n",
    "plankton_clean = plankton_clean.drop(columns=['Size'])\n",
    "\n",
    "plankton_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSeg2003', 'CBSeg2003Description', 'Station', 'Latitude', 'Longitude',\n",
       "       'FieldActivityId', 'SampleDate', 'SampleTime', 'Layer', 'TotalDepth',\n",
       "       'ReportingValue', 'SampleNumber', 'TSN', 'LatinName', 'Method',\n",
       "       'SerialNumber', 'ProjectIdentifier', 'Units', 'DataType',\n",
       "       'SampleVolume', 'PDepth', 'Salzone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plankton_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pivot the `LatinName` column and `ReportingValue` column. We also drop `TSN` since we are using `LatinName` (the other option would be to also pivot on `TSN` and `ReportedValue`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the other columns the same (for now)\n",
    "other_columns = [col for col in plankton_clean.columns if col not in ['LatinName', 'ReportingValue']]\n",
    "\n",
    "# Reset index to use row numbers as the index\n",
    "df_reset = plankton_clean.reset_index(drop=True)\n",
    "\n",
    "# Pivot the DataFrame while preserving non-pivoted columns\n",
    "plankton_pivoted = df_reset.pivot_table(index=df_reset.index, columns='LatinName', values='ReportingValue', aggfunc='first')\n",
    "\n",
    "# Combine pivoted result with the original DataFrame columns not involved in the pivot\n",
    "plankton_pivoted= df_reset.drop(columns=['LatinName','ReportingValue']).join(plankton_pivoted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93466, 672)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plankton_pivoted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine rows if they agree on all of the columns in the monitoring data dictionary (Except those that we dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4737, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_group = ['CBSeg2003', 'CBSeg2003Description', 'DataType',\n",
    "       'SampleDate', 'Layer', 'Latitude', 'Longitude', 'PDepth', 'Salzone',\n",
    "       'SampleVolume', 'Units', 'Station', 'TotalDepth', 'SampleTime',\n",
    "       'ProjectIdentifier']\n",
    "\n",
    "#check unique combinations\n",
    "#this allows us to check that we havent lost data\n",
    "unique_combinations = plankton_pivoted[columns_to_group].drop_duplicates()\n",
    "\n",
    "\n",
    "unique_combinations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003 True\n",
      "CBSeg2003Description True\n",
      "DataType True\n",
      "SampleDate True\n",
      "Layer True\n",
      "Latitude True\n",
      "Longitude True\n",
      "PDepth True\n",
      "Salzone True\n",
      "SampleVolume True\n",
      "Units True\n",
      "Station True\n",
      "TotalDepth True\n",
      "SampleTime True\n",
      "ProjectIdentifier True\n"
     ]
    }
   ],
   "source": [
    "# check we haven't lost unique values\n",
    "for col in columns_to_group:\n",
    "    print(col, unique_combinations[col].unique().size == \n",
    "          plankton_clean[col].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame for processing\n",
    "plankton_processed = plankton_pivoted.copy()\n",
    "\n",
    "# Create a unique identifier for each group based on the columns to match\n",
    "plankton_processed['UniqueID'] = plankton_processed[columns_to_group].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Group by the unique identifier\n",
    "plankton_combined = plankton_processed.groupby('UniqueID', as_index=False).first()\n",
    "\n",
    "# Drop the UniqueID column and remove duplicates\n",
    "plankton_really_clean = plankton_combined.drop(columns='UniqueID').drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSeg2003 True\n",
      "CBSeg2003Description True\n",
      "DataType True\n",
      "SampleDate True\n",
      "Layer True\n",
      "Latitude True\n",
      "Longitude True\n",
      "PDepth True\n",
      "Salzone True\n",
      "SampleVolume True\n",
      "Units True\n",
      "Station True\n",
      "TotalDepth True\n",
      "SampleTime True\n",
      "ProjectIdentifier True\n"
     ]
    }
   ],
   "source": [
    "# Check we haven't lost unique non-empty values\n",
    "for col in columns_to_group:\n",
    "    # Filter out empty values\n",
    "    df_combined_nonempty = plankton_really_clean[col].dropna()\n",
    "    plankton_pivoted_nonempty = plankton[col].dropna()\n",
    "    \n",
    "    # Check if the number of unique non-empty values matches\n",
    "    unique_check = df_combined_nonempty.unique().size == plankton_pivoted_nonempty.unique().size\n",
    "    \n",
    "    # Print the results\n",
    "    print(col, unique_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton_really_clean.to_csv('../data/plank_ChesapeakeBayPlankton_clean.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
